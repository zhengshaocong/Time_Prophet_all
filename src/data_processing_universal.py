# -*- coding: utf-8 -*-
"""
é€šç”¨æ•°æ®é¢„å¤„ç†ä¸»ç¨‹åº
æ”¯æŒæ•°æ®æºç‰¹åŒ–æ¶æ„ï¼Œä¸»ç¨‹åºåªè´Ÿè´£æµç¨‹æ§åˆ¶ï¼Œå…·ä½“å¤„ç†é€»è¾‘ç”±ç‰¹åŒ–æ¨¡å—å®ç°
"""

import pandas as pd
import numpy as np
import json
import sys
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from config import DATA_DIR, OUTPUT_DATA_DIR
from utils.interactive_utils import print_header, print_success, print_error, print_info, print_warning
from utils.file_utils import write_csv, write_json


class UniversalDataProcessor:
    """é€šç”¨æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é€šç”¨æ•°æ®å¤„ç†å™¨"""
        self.data = None
        self.processed_data = None
        self.data_source_name = None
        self.config = None
        self.processing_log = []
        
    def detect_data_source(self, file_path):
        """
        æ£€æµ‹æ•°æ®æº
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ•°æ®æºåç§°
        """
        file_path = Path(file_path)
        data_source_name = file_path.stem
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„ç‰¹åŒ–é…ç½®
        config_path = DATA_DIR / data_source_name / "config.json"
        if config_path.exists():
            print_success(f"æ£€æµ‹åˆ°æ•°æ®æº: {data_source_name}")
            return data_source_name
        else:
            print_warning(f"æœªæ‰¾åˆ°æ•°æ®æº {data_source_name} çš„ç‰¹åŒ–é…ç½®")
            return None
    
    def load_specialized_config(self, data_source_name):
        """
        åŠ è½½ç‰¹åŒ–é…ç½®
        
        Args:
            data_source_name: æ•°æ®æºåç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½é…ç½®
        """
        config_path = DATA_DIR / data_source_name / "config.json"
        
        if not config_path.exists():
            print_error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
            
            print_success(f"å·²åŠ è½½ç‰¹åŒ–é…ç½®: {config_path}")
            print_info(f"æ•°æ®æº: {self.config.get('data_source_name', 'unknown')}")
            print_info(f"æè¿°: {self.config.get('description', 'æ— æè¿°')}")
            
            return True
            
        except Exception as e:
            print_error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def load_specialized_modules(self, data_source_name):
        """
        åŠ è½½ç‰¹åŒ–æ¨¡å—
        
        Args:
            data_source_name: æ•°æ®æºåç§°
            
        Returns:
            tuple: (ç‰¹å¾å·¥ç¨‹å™¨, å¼‚å¸¸æ£€æµ‹å™¨)
        """
        try:
            # åŠ¨æ€å¯¼å…¥ç‰¹åŒ–æ¨¡å—
            module_path = DATA_DIR / data_source_name
            
            # æ·»åŠ æ¨¡å—è·¯å¾„åˆ°sys.path
            if str(module_path) not in sys.path:
                sys.path.insert(0, str(module_path))
            
            # å¯¼å…¥ç‰¹åŒ–æ¨¡å—
            from features import FeatureEngineer
            from anomalies import AnomalyDetector
            
            # åˆ›å»ºç‰¹åŒ–å¤„ç†å™¨
            feature_engineer = FeatureEngineer(self.config)
            anomaly_detector = AnomalyDetector(self.config)
            
            print_success(f"å·²åŠ è½½ç‰¹åŒ–æ¨¡å—: {data_source_name}")
            return feature_engineer, anomaly_detector
            
        except ImportError as e:
            print_error(f"å¯¼å…¥ç‰¹åŒ–æ¨¡å—å¤±è´¥: {e}")
            print_info("è¯·ç¡®ä¿ç‰¹åŒ–æ¨¡å—æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
            return None, None
        except Exception as e:
            print_error(f"åŠ è½½ç‰¹åŒ–æ¨¡å—å¤±è´¥: {e}")
            return None, None
    
    def load_data(self, file_path):
        """
        åŠ è½½æ•°æ®
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("æ•°æ®åŠ è½½")
        
        try:
            # æ£€æµ‹æ•°æ®æº
            self.data_source_name = self.detect_data_source(file_path)
            if not self.data_source_name:
                return False
            
            # åŠ è½½ç‰¹åŒ–é…ç½®
            if not self.load_specialized_config(self.data_source_name):
                return False
            
            # åŠ è½½æ•°æ®
            print_info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
            self.data = pd.read_csv(file_path)
            
            print_success(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.data):,} æ¡è®°å½•, {len(self.data.columns)} ä¸ªå­—æ®µ")
            
            # è®°å½•å¤„ç†æ—¥å¿—
            self.processing_log.append({
                "æ­¥éª¤": "æ•°æ®åŠ è½½",
                "æ•°æ®æº": self.data_source_name,
                "è®°å½•æ•°": len(self.data),
                "å­—æ®µæ•°": len(self.data.columns),
                "æ—¶é—´": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            print_error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def basic_data_cleaning(self):
        """
        åŸºç¡€æ•°æ®æ¸…æ´—ï¼ˆé€šç”¨é€»è¾‘ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("åŸºç¡€æ•°æ®æ¸…æ´—")
        
        if self.data is None:
            print_error("è¯·å…ˆåŠ è½½æ•°æ®")
            return False
        
        try:
            data = self.data.copy()
            original_count = len(data)
            
            # 1. å¤„ç†ç¼ºå¤±å€¼
            print_info("å¤„ç†ç¼ºå¤±å€¼...")
            missing_counts = data.isnull().sum()
            if missing_counts.sum() > 0:
                print(f"  å‘ç°ç¼ºå¤±å€¼:")
                for col, count in missing_counts[missing_counts > 0].items():
                    print(f"    {col}: {count} ä¸ª")
                
                # æ•°å€¼å­—æ®µç”¨0å¡«å……ï¼Œå­—ç¬¦ä¸²å­—æ®µç”¨ç©ºå­—ç¬¦ä¸²å¡«å……
                numeric_columns = data.select_dtypes(include=[np.number]).columns
                string_columns = data.select_dtypes(include=['object']).columns
                
                data[numeric_columns] = data[numeric_columns].fillna(0)
                data[string_columns] = data[string_columns].fillna('')
                
                print("  å·²å¡«å……ç¼ºå¤±å€¼")
            else:
                print("  æ— ç¼ºå¤±å€¼")
            
            # 2. æ•°æ®ç±»å‹è½¬æ¢
            print_info("è½¬æ¢æ•°æ®ç±»å‹...")
            field_mapping = self.config.get("field_mapping", {})
            
            # è½¬æ¢æ—¶é—´å­—æ®µ
            time_field = field_mapping.get("æ—¶é—´å­—æ®µ")
            if time_field and time_field in data.columns:
                time_format = field_mapping.get("æ—¶é—´æ ¼å¼", "%Y%m%d")
                try:
                    data[time_field] = pd.to_datetime(data[time_field], format=time_format)
                    print(f"  å·²è½¬æ¢æ—¶é—´å­—æ®µ: {time_field}")
                except Exception as e:
                    print_warning(f"æ—¶é—´å­—æ®µè½¬æ¢å¤±è´¥: {e}")
            
            # 3. æ—¶é—´èŒƒå›´è¿‡æ»¤
            print_info("åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤...")
            data = self._apply_time_range_filter(data)
            
            # 4. è½¬æ¢æ•°å€¼å­—æ®µ
            numeric_fields = self.config.get("data_validation", {}).get("æ•°å€¼å­—æ®µ", [])
            for field in numeric_fields:
                if field in data.columns:
                    try:
                        data[field] = pd.to_numeric(data[field], errors='coerce')
                        data[field] = data[field].fillna(0)
                        print(f"  å·²è½¬æ¢æ•°å€¼å­—æ®µ: {field}")
                    except Exception as e:
                        print_warning(f"æ•°å€¼å­—æ®µè½¬æ¢å¤±è´¥ {field}: {e}")
            
            # 5. æ•°æ®èšåˆ
            data = self._aggregate_data(data)
            
            self.data = data
            cleaned_count = len(data)
            
            print_success(f"åŸºç¡€æ•°æ®æ¸…æ´—å®Œæˆ: {original_count:,} -> {cleaned_count:,} æ¡")
            
            self.processing_log.append({
                "æ­¥éª¤": "åŸºç¡€æ•°æ®æ¸…æ´—",
                "åŸå§‹æ•°æ®é‡": original_count,
                "æ¸…æ´—åæ•°æ®é‡": cleaned_count,
                "æ—¶é—´": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            print_error(f"åŸºç¡€æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return False
    
    def _apply_time_range_filter(self, data):
        """
        åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: è¿‡æ»¤åçš„æ•°æ®
        """
        # è·å–æ—¶é—´èŒƒå›´é…ç½®
        time_range_config = self.config.get("data_preprocessing", {}).get("æ—¶é—´èŒƒå›´é™åˆ¶", {})
        
        if not time_range_config.get("å¯ç”¨æ—¶é—´èŒƒå›´é™åˆ¶", False):
            print_info("æ—¶é—´èŒƒå›´é™åˆ¶æœªå¯ç”¨ï¼Œè·³è¿‡è¿‡æ»¤")
            return data
        
        field_mapping = self.config.get("field_mapping", {})
        time_field = field_mapping.get("æ—¶é—´å­—æ®µ")
        
        if not time_field or time_field not in data.columns:
            print_warning(f"æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨ï¼Œè·³è¿‡æ—¶é—´èŒƒå›´è¿‡æ»¤")
            return data
        
        try:
            # ç¡®ä¿æ—¶é—´å­—æ®µæ˜¯datetimeç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(data[time_field]):
                data[time_field] = pd.to_datetime(data[time_field])
            
            # è·å–åŸå§‹æ•°æ®çš„æ—¶é—´èŒƒå›´
            original_start = data[time_field].min()
            original_end = data[time_field].max()
            
            # è§£æé…ç½®ä¸­çš„æ—¶é—´èŒƒå›´
            config_start = pd.to_datetime(time_range_config.get("å¼€å§‹æ—¥æœŸ", "2014-01-01"))
            config_end = pd.to_datetime(time_range_config.get("ç»“æŸæ—¥æœŸ", "2014-12-31"))
            
            # æ£€æŸ¥é…ç½®çš„æ—¶é—´èŒƒå›´æ˜¯å¦è¶…å‡ºåŸå§‹æ•°æ®èŒƒå›´
            if time_range_config.get("è¶…å‡ºèŒƒå›´è­¦å‘Š", True):
                if config_start < original_start:
                    print_warning(f"é…ç½®çš„å¼€å§‹æ—¥æœŸ {config_start.date()} æ—©äºæ•°æ®æœ€æ—©æ—¥æœŸ {original_start.date()}")
                if config_end > original_end:
                    print_warning(f"é…ç½®çš„ç»“æŸæ—¥æœŸ {config_end.date()} æ™šäºæ•°æ®æœ€æ™šæ—¥æœŸ {original_end.date()}")
            
            # è‡ªåŠ¨è°ƒæ•´æ—¶é—´èŒƒå›´
            if time_range_config.get("è‡ªåŠ¨è°ƒæ•´", False):
                effective_start = max(config_start, original_start)
                effective_end = min(config_end, original_end)
                print_info(f"è‡ªåŠ¨è°ƒæ•´æ—¶é—´èŒƒå›´: {effective_start.date()} åˆ° {effective_end.date()}")
            else:
                effective_start = config_start
                effective_end = config_end
            
            # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
            filtered_data = data[(data[time_field] >= effective_start) & 
                                (data[time_field] <= effective_end)]
            
            filtered_count = len(filtered_data)
            original_count = len(data)
            
            print_info(f"æ—¶é—´èŒƒå›´è¿‡æ»¤: {original_count:,} -> {filtered_count:,} æ¡")
            print_info(f"æ—¶é—´èŒƒå›´: {effective_start.date()} åˆ° {effective_end.date()}")
            
            if filtered_count == 0:
                print_warning("è¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´é…ç½®")
            
            return filtered_data
            
        except Exception as e:
            print_error(f"æ—¶é—´èŒƒå›´è¿‡æ»¤å¤±è´¥: {e}")
            return data
    
    def _aggregate_data(self, data):
        """
        æ•°æ®èšåˆ
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: èšåˆåçš„æ•°æ®
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰èšåˆé…ç½®
        aggregate_config = self.config.get("data_preprocessing", {}).get("æ•°æ®èšåˆ", {})
        
        if not aggregate_config.get("å¯ç”¨èšåˆ", False):
            print_info("è·³è¿‡æ•°æ®èšåˆ (èšåˆåŠŸèƒ½æœªå¯ç”¨)")
            print(f"  â­ï¸  èšåˆåŠŸèƒ½å·²ç¦ç”¨ï¼Œä¿æŒåŸå§‹æ•°æ®é‡: {len(data):,} æ¡")
            return data
        
        print_info("å¼€å§‹æ‰§è¡Œæ•°æ®èšåˆ...")
        
        # è·å–èšåˆé…ç½®
        time_field = aggregate_config.get("æ—¶é—´å­—æ®µ", "auto")
        aggregate_method = aggregate_config.get("èšåˆæ–¹å¼", "daily")
        aggregate_function = aggregate_config.get("èšåˆå‡½æ•°", "sum")
        
        print(f"  ğŸ”„ èšåˆåŠŸèƒ½å·²å¯ç”¨")
        print(f"  ğŸ“… èšåˆæ–¹å¼: {aggregate_method}")
        print(f"  ğŸ§® èšåˆå‡½æ•°: {aggregate_function}")
        print(f"  ğŸ•’ æ—¶é—´å­—æ®µ: {time_field}")
        
        # è‡ªåŠ¨æ£€æµ‹æ—¶é—´å­—æ®µ
        if time_field == "auto":
            time_fields = [col for col in data.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'æ—¶é—´', 'æ—¥æœŸ'])]
            if time_fields:
                time_field = time_fields[0]
                print_info(f"è‡ªåŠ¨æ£€æµ‹åˆ°æ—¶é—´å­—æ®µ: {time_field}")
            else:
                print_warning("æœªæ£€æµ‹åˆ°æ—¶é—´å­—æ®µï¼Œè·³è¿‡èšåˆ")
                print(f"  âŒ èšåˆå¤±è´¥: æœªæ‰¾åˆ°æ—¶é—´å­—æ®µ")
                print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ—¶é—´ç›¸å…³å­—æ®µï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šæ—¶é—´å­—æ®µ")
                return data
        elif time_field not in data.columns:
            print_warning(f"æ—¶é—´å­—æ®µ {time_field} ä¸å­˜åœ¨ï¼Œè·³è¿‡èšåˆ")
            print(f"  âŒ èšåˆå¤±è´¥: æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ—¶é—´å­—æ®µåç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ 'auto' è‡ªåŠ¨æ£€æµ‹")
            return data
        
        try:
            # ç¡®ä¿æ—¶é—´å­—æ®µæ˜¯datetimeç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(data[time_field]):
                data[time_field] = pd.to_datetime(data[time_field])
                print(f"  ğŸ”„ å·²è½¬æ¢æ—¶é—´å­—æ®µ '{time_field}' ä¸ºdatetimeç±»å‹")
            
            # è®¾ç½®æ—¶é—´ç´¢å¼•
            data_temp = data.set_index(time_field)
            
            # æ ¹æ®èšåˆæ–¹å¼é€‰æ‹©é‡é‡‡æ ·é¢‘ç‡
            if aggregate_method == "daily":
                freq = "D"
            elif aggregate_method == "weekly":
                freq = "W"
            elif aggregate_method == "monthly":
                freq = "M"
            else:
                print_warning(f"ä¸æ”¯æŒçš„èšåˆæ–¹å¼: {aggregate_method}ï¼Œè·³è¿‡èšåˆ")
                print(f"  âŒ èšåˆå¤±è´¥: ä¸æ”¯æŒçš„èšåˆæ–¹å¼ '{aggregate_method}'")
                print(f"  ğŸ’¡ æ”¯æŒçš„èšåˆæ–¹å¼: daily, weekly, monthly")
                return data
            
            # é€‰æ‹©èšåˆå­—æ®µ
            aggregate_columns = aggregate_config.get("èšåˆå­—æ®µ", "auto")
            if aggregate_columns == "auto":
                # è‡ªåŠ¨æ£€æµ‹æ•°å€¼åˆ—
                numeric_columns = data_temp.select_dtypes(include=[np.number]).columns
                exclude_columns = aggregate_config.get("æ’é™¤å­—æ®µ", ['user_id'])
                aggregate_columns = [col for col in numeric_columns if col not in exclude_columns]
                print(f"  ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ° {len(aggregate_columns)} ä¸ªæ•°å€¼å­—æ®µç”¨äºèšåˆ")
            else:
                # ä½¿ç”¨æŒ‡å®šçš„èšåˆå­—æ®µ
                aggregate_columns = [col for col in aggregate_columns if col in data_temp.columns]
                print(f"  ğŸ“‹ ä½¿ç”¨æŒ‡å®šçš„ {len(aggregate_columns)} ä¸ªå­—æ®µè¿›è¡Œèšåˆ")
            
            if not aggregate_columns:
                print_warning("æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„æ•°å€¼åˆ—")
                print(f"  âŒ èšåˆå¤±è´¥: æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„æ•°å€¼åˆ—")
                print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ•°å€¼å­—æ®µï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šèšåˆå­—æ®µ")
                return data
            
            # æ‰§è¡Œèšåˆ
            print(f"  ğŸš€ å¼€å§‹æ‰§è¡Œèšåˆæ“ä½œ...")
            if aggregate_function == "sum":
                aggregated = data_temp[aggregate_columns].resample(freq).sum()
            elif aggregate_function == "mean":
                aggregated = data_temp[aggregate_columns].resample(freq).mean()
            elif aggregate_function == "median":
                aggregated = data_temp[aggregate_columns].resample(freq).median()
            elif aggregate_function == "max":
                aggregated = data_temp[aggregate_columns].resample(freq).max()
            elif aggregate_function == "min":
                aggregated = data_temp[aggregate_columns].resample(freq).min()
            else:
                print_warning(f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {aggregate_function}ï¼Œä½¿ç”¨sum")
                aggregated = data_temp[aggregate_columns].resample(freq).sum()
            
            # å¤„ç†èšåˆåçš„ç¼ºå¤±å€¼
            if aggregate_config.get("å¤„ç†ç¼ºå¤±å€¼", True):
                missing_fill = aggregate_config.get("ç¼ºå¤±å€¼å¡«å……", 0)
                aggregated = aggregated.fillna(missing_fill)
                print(f"  ğŸ”§ å·²å¤„ç†èšåˆåçš„ç¼ºå¤±å€¼ (å¡«å……å€¼: {missing_fill})")
            
            # é‡ç½®ç´¢å¼•
            if aggregate_config.get("è¾“å‡ºæ ¼å¼", {}).get("é‡ç½®ç´¢å¼•", True):
                aggregated = aggregated.reset_index()
                print(f"  ğŸ”„ å·²é‡ç½®æ—¶é—´ç´¢å¼•")
            
            # èšåˆåçš„æç¤º
            if len(aggregated) != len(data):
                print(f"  âœ… èšåˆå®Œæˆ: {len(data):,} -> {len(aggregated):,} æ¡ (å‡å°‘ {((len(data) - len(aggregated)) / len(data) * 100):.1f}%)")
            else:
                print(f"  âš ï¸  èšåˆæœªç”Ÿæ•ˆ: æ•°æ®é‡æœªå˜åŒ– ({len(data):,} -> {len(aggregated):,} æ¡)")
            
            print(f"  âœ… èšåˆæ“ä½œå®Œæˆ!")
            return aggregated
            
        except Exception as e:
            print_error(f"æ•°æ®èšåˆå¤±è´¥: {e}")
            print(f"  âŒ èšåˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®æ ¼å¼ã€æ—¶é—´å­—æ®µæ ¼å¼ã€èšåˆé…ç½®ç­‰")
            return data
    
    def specialized_processing(self):
        """
        ç‰¹åŒ–å¤„ç†ï¼ˆè°ƒç”¨ç‰¹åŒ–æ¨¡å—ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("ç‰¹åŒ–å¤„ç†")
        
        if self.data is None:
            print_error("è¯·å…ˆå®ŒæˆåŸºç¡€æ•°æ®æ¸…æ´—")
            return False
        
        try:
            # åŠ è½½ç‰¹åŒ–æ¨¡å—
            feature_engineer, anomaly_detector = self.load_specialized_modules(self.data_source_name)
            if feature_engineer is None or anomaly_detector is None:
                return False
            
            # 1. å¼‚å¸¸æ£€æµ‹
            print_info("æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
            clean_data, anomalies = anomaly_detector.detect_anomalies(self.data)
            
            # ä¿å­˜å¼‚å¸¸æ£€æµ‹ç»“æœåˆ°å®ä¾‹å˜é‡
            self.anomalies = anomalies
            
            # æ˜¾ç¤ºå¼‚å¸¸æ£€æµ‹ç»“æœ
            anomaly_summary = anomaly_detector.get_anomaly_summary(anomalies)
            print(f"  å¼‚å¸¸æ£€æµ‹å®Œæˆ: {anomaly_summary.get('æ€»å¼‚å¸¸è®°å½•æ•°', 0)} æ¡å¼‚å¸¸è®°å½•")
            
            # 2. ç‰¹å¾å·¥ç¨‹
            print_info("æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
            data_with_features = feature_engineer.engineer_features(clean_data)
            
            # æ˜¾ç¤ºç‰¹å¾å·¥ç¨‹ç»“æœ
            original_columns = len(self.data.columns)
            new_columns = len(data_with_features.columns)
            print(f"  ç‰¹å¾å·¥ç¨‹å®Œæˆ: {original_columns} -> {new_columns} åˆ—")
            print(f"  æ–°å¢ç‰¹å¾: {new_columns - original_columns} ä¸ª")
            
            self.processed_data = data_with_features
            
            # è®°å½•å¤„ç†æ—¥å¿—
            self.processing_log.append({
                "æ­¥éª¤": "ç‰¹åŒ–å¤„ç†",
                "å¼‚å¸¸æ£€æµ‹": {
                    "å¼‚å¸¸ç±»å‹æ•°": anomaly_summary.get("æ€»å¼‚å¸¸ç±»å‹æ•°", 0),
                    "å¼‚å¸¸è®°å½•æ•°": anomaly_summary.get("æ€»å¼‚å¸¸è®°å½•æ•°", 0)
                },
                "ç‰¹å¾å·¥ç¨‹": {
                    "åŸå§‹åˆ—æ•°": original_columns,
                    "æ–°åˆ—æ•°": new_columns,
                    "æ–°å¢ç‰¹å¾æ•°": new_columns - original_columns
                },
                "æ—¶é—´": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            print_error(f"ç‰¹åŒ–å¤„ç†å¤±è´¥: {e}")
            return False
    
    def save_results(self, output_path=None):
        """
        ä¿å­˜å¤„ç†ç»“æœ
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.processed_data is None:
            print_error("æ²¡æœ‰å¤„ç†åçš„æ•°æ®å¯ä¿å­˜")
            return False
        
        if output_path is None:
            # ä½¿ç”¨è¦†ç›–æ¨¡å¼ï¼Œä¸æ·»åŠ æ—¶é—´æˆ³
            output_path = OUTPUT_DATA_DIR / f"{self.data_source_name}_processed.csv"
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ•°æ®
            self.processed_data.to_csv(output_path, index=False, encoding='utf-8')
            print_success(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {output_path}")
            
            # ä¿å­˜å¤„ç†æ—¥å¿—
            log_path = output_path.parent / f"{self.data_source_name}_processing_log.json"
            if write_json(self.processing_log, log_path):
                print_info(f"å¤„ç†æ—¥å¿—å·²ä¿å­˜: {log_path}")
            else:
                print_warning(f"å¤„ç†æ—¥å¿—ä¿å­˜å¤±è´¥: {log_path}")
            
            # ä¿å­˜å¼‚å¸¸æ£€æµ‹ç»“æœ
            if hasattr(self, 'anomalies') and self.anomalies:
                anomaly_path = output_path.parent / f"{self.data_source_name}_anomalies.json"
                if write_json(self.anomalies, anomaly_path):
                    print_info(f"å¼‚å¸¸æ£€æµ‹ç»“æœå·²ä¿å­˜: {anomaly_path}")
                else:
                    print_warning(f"å¼‚å¸¸æ£€æµ‹ç»“æœä¿å­˜å¤±è´¥: {anomaly_path}")
            
            return True
            
        except Exception as e:
            print_error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def run_full_pipeline(self, file_path):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("é€šç”¨æ•°æ®å¤„ç†æµæ°´çº¿", "æ•°æ®åŠ è½½ -> åŸºç¡€æ¸…æ´— -> ç‰¹åŒ–å¤„ç† -> ç»“æœä¿å­˜")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data(file_path):
            return False
        
        # 2. åŸºç¡€æ•°æ®æ¸…æ´—
        if not self.basic_data_cleaning():
            return False
        
        # 3. ç‰¹åŒ–å¤„ç†
        if not self.specialized_processing():
            return False
        
        # 4. ä¿å­˜ç»“æœ
        if not self.save_results():
            return False
        
        print_success("å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        print(f"å¤„ç†æ—¥å¿—åŒ…å« {len(self.processing_log)} ä¸ªæ­¥éª¤")
        
        return True
    
    def get_processing_summary(self):
        """
        è·å–å¤„ç†æ‘˜è¦
        
        Returns:
            dict: å¤„ç†æ‘˜è¦
        """
        if not self.processing_log:
            return {}
        
        return {
            "æ•°æ®æº": self.data_source_name,
            "å¤„ç†æ­¥éª¤æ•°": len(self.processing_log),
            "åŸå§‹æ•°æ®é‡": self.processing_log[0].get("è®°å½•æ•°", 0) if self.processing_log else 0,
            "æœ€ç»ˆæ•°æ®é‡": len(self.processed_data) if self.processed_data is not None else 0,
            "æœ€ç»ˆç‰¹å¾æ•°": len(self.processed_data.columns) if self.processed_data is not None else 0,
            "å¤„ç†æ—¶é—´": self.processing_log[-1].get("æ—¶é—´", "") if self.processing_log else ""
        }


def run_universal_data_processing(file_path=None):
    """è¿è¡Œé€šç”¨æ•°æ®å¤„ç†åŠŸèƒ½"""
    print_header("é€šç”¨æ•°æ®å¤„ç†æ¨¡å—", "æ”¯æŒæ•°æ®æºç‰¹åŒ–æ¶æ„")
    
    if file_path is None:
        # è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        data_files = list(DATA_DIR.glob("*/*.csv"))
        if not data_files:
            print_error("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            return False
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ•°æ®æ–‡ä»¶
        file_path = data_files[0]
        print_info(f"è‡ªåŠ¨é€‰æ‹©æ•°æ®æ–‡ä»¶: {file_path}")
    
    # åˆ›å»ºé€šç”¨æ•°æ®å¤„ç†å™¨
    processor = UniversalDataProcessor()
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    success = processor.run_full_pipeline(file_path)
    
    if success:
        # æ˜¾ç¤ºå¤„ç†æ‘˜è¦
        summary = processor.get_processing_summary()
        print("\n" + "="*50)
        print("å¤„ç†æ‘˜è¦:")
        for key, value in summary.items():
            print(f"  {key}: {value}")
        print("="*50)
    else:
        print_error("æ•°æ®å¤„ç†å¤±è´¥ï¼")
    
    return success


if __name__ == "__main__":
    run_universal_data_processing() 