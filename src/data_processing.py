# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†æ¨¡å—
åŒ…å«æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®è½¬æ¢ç­‰å®Œæ•´çš„æ•°æ®å¤„ç†åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from config import (
    DATA_DIR, OUTPUT_DATA_DIR, DATA_PREPROCESSING_CONFIG, 
    DATA_PROCESSING_CONFIG, FEATURE_ENGINEERING_CONFIG, 
    DATA_TRANSFORMATION_CONFIG, DEFAULT_FIELD_MAPPING
)
from utils.interactive_utils import print_header, print_success, print_error, print_info, print_warning
from utils.file_utils import write_csv, write_json
from utils.data_processor import DataProcessor



class DataProcessingPipeline:
    """æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†æµæ°´çº¿
        
        Args:
            config: å¤„ç†é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or DATA_PROCESSING_CONFIG
        self.preprocessing_config = DATA_PREPROCESSING_CONFIG
        self.feature_config = FEATURE_ENGINEERING_CONFIG
        self.transformation_config = DATA_TRANSFORMATION_CONFIG
        self.field_mapping = DEFAULT_FIELD_MAPPING
        self.data = None
        self.processed_data = None
        self.processing_log = []
        
    def load_and_analyze_data(self, file_path=None):
        """
        åŠ è½½å¹¶åˆ†ææ•°æ®
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("æ•°æ®åŠ è½½ä¸åˆ†æ")
        
        if file_path is None:
            file_path = DATA_DIR / "user_balance_table.csv"
        
        # ä¿å­˜æ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_file_path = Path(file_path)
        
        try:
            # ä½¿ç”¨åŸºç¡€æ•°æ®å¤„ç†å™¨åŠ è½½æ•°æ®
            processor = DataProcessor()
            if not processor.load_data(file_path):
                return False
            
            # åˆ†ææ•°æ®ç»“æ„
            if not processor.analyze_data_structure():
                return False
            
            self.data = processor.data
            print_success("æ•°æ®åŠ è½½ä¸åˆ†æå®Œæˆ")
            return True
            
        except Exception as e:
            print_error(f"æ•°æ®åŠ è½½ä¸åˆ†æå¤±è´¥: {e}")
            return False
    
    def clean_data(self):
        """
        æ•°æ®æ¸…æ´—
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("æ•°æ®æ¸…æ´—")
        
        if self.data is None:
            print_error("è¯·å…ˆåŠ è½½æ•°æ®")
            return False
        
        try:
            data = self.data.copy()
            original_count = len(data)
            
            # 1. æ•°æ®ç±»å‹è½¬æ¢
            print_info("è½¬æ¢æ•°æ®ç±»å‹...")
            data = self._convert_data_types(data)
            
            # 2. æ—¶é—´å­—æ®µå¤„ç†
            print_info("å¤„ç†æ—¶é—´å­—æ®µ...")
            data = self._process_time_field(data)
            
            # 3. æ—¶é—´èŒƒå›´è¿‡æ»¤
            print_info("åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤...")
            data = self._apply_time_range_filter(data)
            
            # 4. å¤„ç†ç¼ºå¤±å€¼
            print_info("å¤„ç†ç¼ºå¤±å€¼...")
            missing_config = self.preprocessing_config["ç¼ºå¤±å€¼å¤„ç†"]
            for field, fill_value in missing_config.items():
                if field in data.columns:
                    data[field] = data[field].fillna(fill_value)
                    print(f"  {field}: å¡«å……ç¼ºå¤±å€¼ä¸º {fill_value}")
            
            # 5. å¤„ç†å¼‚å¸¸å€¼
            if self.preprocessing_config["å¼‚å¸¸å€¼å¤„ç†"]["å¯ç”¨å¼‚å¸¸å€¼æ£€æµ‹"]:
                print_info("å¤„ç†å¼‚å¸¸å€¼...")
                data = self._handle_outliers(data)
            
            # 5. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
            print_info("æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§...")
            data = self._check_data_consistency(data)
            
            # 6. æ•°æ®èšåˆ
            aggregate_config = self.preprocessing_config["æ•°æ®èšåˆ"]
            if aggregate_config["å¯ç”¨èšåˆ"]:
                print_info("æ‰§è¡Œæ•°æ®èšåˆ...")
                print(f"  ğŸ”„ èšåˆåŠŸèƒ½å·²å¯ç”¨")
                print(f"  ğŸ“… èšåˆæ–¹å¼: {aggregate_config['èšåˆæ–¹å¼']}")
                print(f"  ğŸ§® èšåˆå‡½æ•°: {aggregate_config['èšåˆå‡½æ•°']}")
                print(f"  ğŸ•’ æ—¶é—´å­—æ®µ: {aggregate_config['æ—¶é—´å­—æ®µ']}")
                data = self._aggregate_data(data)
                # èšåˆåçš„æç¤º
                if len(data) != original_count:
                    print(f"  âœ… èšåˆå®Œæˆ: {original_count:,} -> {len(data):,} æ¡ (å‡å°‘ {((original_count - len(data)) / original_count * 100):.1f}%)")
                else:
                    print(f"  âš ï¸  èšåˆæœªç”Ÿæ•ˆ: æ•°æ®é‡æœªå˜åŒ– ({original_count:,} -> {len(data):,} æ¡)")
            else:
                print_info("è·³è¿‡æ•°æ®èšåˆ (èšåˆåŠŸèƒ½æœªå¯ç”¨)")
                print(f"  â­ï¸  èšåˆåŠŸèƒ½å·²ç¦ç”¨ï¼Œä¿æŒåŸå§‹æ•°æ®é‡: {len(data):,} æ¡")
            
            self.data = data
            cleaned_count = len(data)
            
            print_success(f"æ•°æ®æ¸…æ´—å®Œæˆ: {original_count:,} -> {cleaned_count:,} æ¡")
            
            # æ·»åŠ èšåˆçŠ¶æ€æ€»ç»“
            if aggregate_config["å¯ç”¨èšåˆ"] and len(data) != original_count:
                print(f"\nğŸ“Š ã€èšåˆçŠ¶æ€æ€»ç»“ã€‘")
                print(f"  åŸå§‹æ•°æ®é‡: {original_count:,} æ¡")
                print(f"  èšåˆåæ•°æ®é‡: {cleaned_count:,} æ¡")
                print(f"  æ•°æ®å‡å°‘: {original_count - cleaned_count:,} æ¡ ({((original_count - cleaned_count) / original_count * 100):.1f}%)")
                print(f"  èšåˆæ–¹å¼: {aggregate_config['èšåˆæ–¹å¼']} | èšåˆå‡½æ•°: {aggregate_config['èšåˆå‡½æ•°']}")
                print(f"  âœ… èšåˆåŠŸèƒ½å·²æˆåŠŸæ‰§è¡Œ")
            elif aggregate_config["å¯ç”¨èšåˆ"]:
                print(f"\nâš ï¸  ã€èšåˆçŠ¶æ€æ€»ç»“ã€‘")
                print(f"  èšåˆåŠŸèƒ½å·²å¯ç”¨ä½†æœªç”Ÿæ•ˆ")
                print(f"  å¯èƒ½åŸå› : æ—¶é—´å­—æ®µæœªæ‰¾åˆ°ã€èšåˆå­—æ®µä¸ºç©ºã€æ•°æ®æ ¼å¼é—®é¢˜ç­‰")
                print(f"  å»ºè®®æ£€æŸ¥: æ—¶é—´å­—æ®µé…ç½®ã€æ•°æ®æ ¼å¼ã€èšåˆå­—æ®µè®¾ç½®")
            else:
                print(f"\nâ­ï¸  ã€èšåˆçŠ¶æ€æ€»ç»“ã€‘")
                print(f"  èšåˆåŠŸèƒ½å·²ç¦ç”¨")
                print(f"  å¦‚éœ€å¯ç”¨èšåˆï¼Œè¯·ä¿®æ”¹ config/data_processing.py ä¸­çš„ 'å¯ç”¨èšåˆ': True")
            
            self.processing_log.append({
                "æ­¥éª¤": "æ•°æ®æ¸…æ´—",
                "åŸå§‹æ•°æ®é‡": original_count,
                "æ¸…æ´—åæ•°æ®é‡": cleaned_count,
                "èšåˆçŠ¶æ€": "å·²å¯ç”¨" if aggregate_config["å¯ç”¨èšåˆ"] else "å·²ç¦ç”¨",
                "èšåˆæ–¹å¼": aggregate_config["èšåˆæ–¹å¼"] if aggregate_config["å¯ç”¨èšåˆ"] else "æ— ",
                "èšåˆå‡½æ•°": aggregate_config["èšåˆå‡½æ•°"] if aggregate_config["å¯ç”¨èšåˆ"] else "æ— ",
                "æ—¶é—´": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            print_error(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return False
    
    def _handle_outliers(self, data):
        """å¤„ç†å¼‚å¸¸å€¼"""
        outlier_config = self.preprocessing_config["å¼‚å¸¸å€¼å¤„ç†"]
        threshold = outlier_config["å¼‚å¸¸å€¼é˜ˆå€¼"]
        method = outlier_config["å¼‚å¸¸å€¼å¤„ç†æ–¹å¼"]
        
        # æ•°å€¼åˆ—
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            if column in ['user_id', 'report_date']:  # è·³è¿‡IDå’Œæ—¶é—´å­—æ®µ
                continue
                
            mean_val = data[column].mean()
            std_val = data[column].std()
            lower_bound = mean_val - threshold * std_val
            upper_bound = mean_val + threshold * std_val
            
            outlier_count = ((data[column] < lower_bound) | (data[column] > upper_bound)).sum()
            
            if outlier_count > 0:
                if method == "clip":
                    data[column] = data[column].clip(lower_bound, upper_bound)
                    print(f"  {column}: æˆªæ–­ {outlier_count} ä¸ªå¼‚å¸¸å€¼")
                elif method == "remove":
                    data = data[~((data[column] < lower_bound) | (data[column] > upper_bound))]
                    print(f"  {column}: åˆ é™¤ {outlier_count} ä¸ªå¼‚å¸¸å€¼")
        
        return data
    
    def _convert_data_types(self, data):
        """è½¬æ¢æ•°æ®ç±»å‹"""
        # è½¬æ¢æ—¶é—´å­—æ®µ
        time_field = self.field_mapping["æ—¶é—´å­—æ®µ"]
        if time_field in data.columns:
            data[time_field] = pd.to_datetime(data[time_field], format=self.field_mapping["æ—¶é—´æ ¼å¼"])
        
        # ç¡®ä¿æ•°å€¼å­—æ®µä¸ºæ•°å€¼ç±»å‹
        numeric_fields = [
            self.field_mapping["ç”³è´­é‡‘é¢å­—æ®µ"],
            self.field_mapping["èµå›é‡‘é¢å­—æ®µ"],
            self.field_mapping["å½“å‰ä½™é¢å­—æ®µ"],
            self.field_mapping["æ˜¨æ—¥ä½™é¢å­—æ®µ"]
        ]
        
        for field in numeric_fields:
            if field in data.columns:
                data[field] = pd.to_numeric(data[field], errors='coerce')
        
        return data
    
    def _apply_time_range_filter(self, data):
        """
        åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: è¿‡æ»¤åçš„æ•°æ®
        """
        time_range_config = self.preprocessing_config["æ—¶é—´èŒƒå›´é™åˆ¶"]
        
        if not time_range_config["å¯ç”¨æ—¶é—´èŒƒå›´é™åˆ¶"]:
            return data
        
        time_field = self.field_mapping["æ—¶é—´å­—æ®µ"]
        if time_field not in data.columns:
            print_warning(f"æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨ï¼Œè·³è¿‡æ—¶é—´èŒƒå›´è¿‡æ»¤")
            return data
        
        try:
            # ç¡®ä¿æ—¶é—´å­—æ®µæ˜¯datetimeç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(data[time_field]):
                data[time_field] = pd.to_datetime(data[time_field])
            
            # è·å–åŸå§‹æ•°æ®çš„æ—¶é—´èŒƒå›´
            original_start = data[time_field].min()
            original_end = data[time_field].max()
            
            # è§£æé…ç½®ä¸­çš„æ—¶é—´èŒƒå›´
            config_start = pd.to_datetime(time_range_config["å¼€å§‹æ—¥æœŸ"])
            config_end = pd.to_datetime(time_range_config["ç»“æŸæ—¥æœŸ"])
            
            # æ£€æŸ¥é…ç½®çš„æ—¶é—´èŒƒå›´æ˜¯å¦è¶…å‡ºåŸå§‹æ•°æ®èŒƒå›´
            if time_range_config["è¶…å‡ºèŒƒå›´è­¦å‘Š"]:
                if config_start < original_start:
                    print_warning(f"é…ç½®çš„å¼€å§‹æ—¥æœŸ {config_start.date()} æ—©äºæ•°æ®æœ€æ—©æ—¥æœŸ {original_start.date()}")
                if config_end > original_end:
                    print_warning(f"é…ç½®çš„ç»“æŸæ—¥æœŸ {config_end.date()} æ™šäºæ•°æ®æœ€æ™šæ—¥æœŸ {original_end.date()}")
            
            # è‡ªåŠ¨è°ƒæ•´æ—¶é—´èŒƒå›´
            if time_range_config["è‡ªåŠ¨è°ƒæ•´"]:
                effective_start = max(config_start, original_start)
                effective_end = min(config_end, original_end)
                print_info(f"è‡ªåŠ¨è°ƒæ•´æ—¶é—´èŒƒå›´: {effective_start.date()} åˆ° {effective_end.date()}")
            else:
                effective_start = config_start
                effective_end = config_end
            
            # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
            filtered_data = data[(data[time_field] >= effective_start) & 
                                (data[time_field] <= effective_end)]
            
            filtered_count = len(filtered_data)
            original_count = len(data)
            
            print_info(f"æ—¶é—´èŒƒå›´è¿‡æ»¤: {original_count:,} -> {filtered_count:,} æ¡")
            print_info(f"æ—¶é—´èŒƒå›´: {effective_start.date()} åˆ° {effective_end.date()}")
            
            if filtered_count == 0:
                print_warning("è¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´é…ç½®")
            
            return filtered_data
            
        except Exception as e:
            print_error(f"æ—¶é—´èŒƒå›´è¿‡æ»¤å¤±è´¥: {e}")
            return data
    
    def _process_time_field(self, data):
        """å¤„ç†æ—¶é—´å­—æ®µ"""
        time_field = self.field_mapping["æ—¶é—´å­—æ®µ"]
        if time_field not in data.columns:
            return data
        
        time_features = self.preprocessing_config["æ—¶é—´ç‰¹å¾"]
        
        # æå–æ—¶é—´ç‰¹å¾
        if time_features["æå–å¹´ä»½"]:
            data['Year'] = data[time_field].dt.year
        if time_features["æå–æœˆä»½"]:
            data['Month'] = data[time_field].dt.month
        if time_features["æå–æ—¥æœŸ"]:
            data['Day'] = data[time_field].dt.day
        if time_features["æå–æ˜ŸæœŸ"]:
            data['Weekday'] = data[time_field].dt.dayofweek
        if time_features["æå–å­£åº¦"]:
            data['Quarter'] = data[time_field].dt.quarter
        
        return data
    
    def _check_data_consistency(self, data):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§å¹¶å¤„ç†å¼‚å¸¸å€¼"""
        consistency_config = self.preprocessing_config["æ•°æ®ä¸€è‡´æ€§å¤„ç†"]
        
        if not consistency_config["å¯ç”¨ä¸€è‡´æ€§æ£€æŸ¥"]:
            return data
        
        processing_method = consistency_config["å¤„ç†æ–¹å¼"]
        original_count = len(data)
        
        # æ£€æŸ¥ä½™é¢å­—æ®µçš„ä¸€è‡´æ€§
        current_balance_field = self.field_mapping["å½“å‰ä½™é¢å­—æ®µ"]
        previous_balance_field = self.field_mapping["æ˜¨æ—¥ä½™é¢å­—æ®µ"]
        
        if current_balance_field in data.columns and previous_balance_field in data.columns:
            # æ£€æŸ¥ä½™é¢æ˜¯å¦ä¸ºè´Ÿæ•°
            if consistency_config["å¤„ç†è´Ÿä½™é¢"]:
                negative_balance = (data[current_balance_field] < 0).sum()
                if negative_balance > 0:
                    print_warning(f"å‘ç° {negative_balance} æ¡è´Ÿä½™é¢è®°å½•")
                    if processing_method == "correct":
                        # å°†è´Ÿä½™é¢è®¾ä¸º0
                        data.loc[data[current_balance_field] < 0, current_balance_field] = 0
                        print(f"  å·²å°†è´Ÿä½™é¢è®°å½•è®¾ä¸º0")
                    elif processing_method == "remove":
                        # åˆ é™¤è´Ÿä½™é¢è®°å½•
                        data = data[data[current_balance_field] >= 0]
                        print(f"  å·²åˆ é™¤è´Ÿä½™é¢è®°å½•")
            
            # æ£€æŸ¥ä½™é¢å˜åŒ–æ˜¯å¦åˆç†
            if consistency_config["å¤„ç†æç«¯ä½™é¢å˜åŒ–"]:
                balance_change = data[current_balance_field] - data[previous_balance_field]
                extreme_threshold = balance_change.quantile(consistency_config["æç«¯å˜åŒ–é˜ˆå€¼"])
                extreme_changes = (abs(balance_change) > extreme_threshold).sum()
                
                if extreme_changes > 0:
                    print_warning(f"å‘ç° {extreme_changes} æ¡æç«¯ä½™é¢å˜åŒ–è®°å½•")
                    if processing_method == "correct":
                        # å¤„ç†æç«¯ä½™é¢å˜åŒ–ï¼šå°†æç«¯å˜åŒ–é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                        extreme_mask = abs(balance_change) > extreme_threshold
                        
                        # å¯¹äºæç«¯å˜åŒ–ï¼Œä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºåˆç†çš„ä½™é¢å˜åŒ–å€¼
                        median_change = balance_change.median()
                        data.loc[extreme_mask, current_balance_field] = (
                            data.loc[extreme_mask, previous_balance_field] + median_change
                        )
                        print(f"  å·²å°†æç«¯ä½™é¢å˜åŒ–è°ƒæ•´ä¸ºä¸­ä½æ•°å˜åŒ–å€¼")
                    elif processing_method == "remove":
                        # åˆ é™¤æç«¯ä½™é¢å˜åŒ–è®°å½•
                        data = data[abs(balance_change) <= extreme_threshold]
                        print(f"  å·²åˆ é™¤æç«¯ä½™é¢å˜åŒ–è®°å½•")
        
        # æ£€æŸ¥æ•°æ®é€»è¾‘ä¸€è‡´æ€§
        if consistency_config["å¤„ç†è´Ÿé‡‘é¢"]:
            purchase_field = self.field_mapping["ç”³è´­é‡‘é¢å­—æ®µ"]
            redemption_field = self.field_mapping["èµå›é‡‘é¢å­—æ®µ"]
            
            if purchase_field in data.columns and redemption_field in data.columns:
                # æ£€æŸ¥æ˜¯å¦æœ‰è´Ÿçš„ç”³è´­æˆ–èµå›é‡‘é¢
                negative_purchase = (data[purchase_field] < 0).sum()
                negative_redemption = (data[redemption_field] < 0).sum()
                
                if negative_purchase > 0:
                    print_warning(f"å‘ç° {negative_purchase} æ¡è´Ÿç”³è´­é‡‘é¢è®°å½•")
                    if processing_method == "correct":
                        data.loc[data[purchase_field] < 0, purchase_field] = 0
                        print(f"  å·²å°†è´Ÿç”³è´­é‡‘é¢è®¾ä¸º0")
                    elif processing_method == "remove":
                        data = data[data[purchase_field] >= 0]
                        print(f"  å·²åˆ é™¤è´Ÿç”³è´­é‡‘é¢è®°å½•")
                
                if negative_redemption > 0:
                    print_warning(f"å‘ç° {negative_redemption} æ¡è´Ÿèµå›é‡‘é¢è®°å½•")
                    if processing_method == "correct":
                        data.loc[data[redemption_field] < 0, redemption_field] = 0
                        print(f"  å·²å°†è´Ÿèµå›é‡‘é¢è®¾ä¸º0")
                    elif processing_method == "remove":
                        data = data[data[redemption_field] >= 0]
                        print(f"  å·²åˆ é™¤è´Ÿèµå›é‡‘é¢è®°å½•")
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        final_count = len(data)
        if final_count != original_count:
            print(f"  æ•°æ®ä¸€è‡´æ€§å¤„ç†å®Œæˆ: {original_count:,} -> {final_count:,} æ¡")
            print(f"  åˆ é™¤äº† {original_count - final_count:,} æ¡å¼‚å¸¸è®°å½•")
        
        return data
    
    def _aggregate_data(self, data):
        """
        æ•°æ®èšåˆ
        """
        print_info("å¼€å§‹æ‰§è¡Œæ•°æ®èšåˆ...")
        aggregate_config = self.preprocessing_config["æ•°æ®èšåˆ"]
        print_info(f"èšåˆé…ç½®: {aggregate_config}")
        
        # è·å–æ—¶é—´å­—æ®µ
        time_field = aggregate_config["æ—¶é—´å­—æ®µ"]
        if time_field == "auto":
            # è‡ªåŠ¨æ£€æµ‹æ—¶é—´å­—æ®µ
            time_fields = [col for col in data.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'æ—¶é—´', 'æ—¥æœŸ'])]
            if time_fields:
                time_field = time_fields[0]
                print_info(f"è‡ªåŠ¨æ£€æµ‹åˆ°æ—¶é—´å­—æ®µ: {time_field}")
            else:
                print_warning("æœªæ£€æµ‹åˆ°æ—¶é—´å­—æ®µï¼Œè·³è¿‡èšåˆ")
                print(f"  âŒ èšåˆå¤±è´¥: æœªæ‰¾åˆ°æ—¶é—´å­—æ®µ")
                print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ—¶é—´ç›¸å…³å­—æ®µï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šæ—¶é—´å­—æ®µ")
                return data
        elif time_field not in data.columns:
            print_warning(f"æ—¶é—´å­—æ®µ {time_field} ä¸å­˜åœ¨ï¼Œè·³è¿‡èšåˆ")
            print(f"  âŒ èšåˆå¤±è´¥: æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ—¶é—´å­—æ®µåç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ 'auto' è‡ªåŠ¨æ£€æµ‹")
            return data
        
        aggregate_method = aggregate_config["èšåˆæ–¹å¼"]
        aggregate_function = aggregate_config["èšåˆå‡½æ•°"]
        
        print_info(f"ä½¿ç”¨èšåˆæ–¹å¼: {aggregate_method}, èšåˆå‡½æ•°: {aggregate_function}")
        
        try:
            # ç¡®ä¿æ—¶é—´å­—æ®µæ˜¯datetimeç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(data[time_field]):
                data[time_field] = pd.to_datetime(data[time_field])
                print(f"  ğŸ”„ å·²è½¬æ¢æ—¶é—´å­—æ®µ '{time_field}' ä¸ºdatetimeç±»å‹")
            
            # è®¾ç½®æ—¶é—´ç´¢å¼•
            data_temp = data.set_index(time_field)
            
            # æ ¹æ®èšåˆæ–¹å¼é€‰æ‹©é‡é‡‡æ ·é¢‘ç‡
            if aggregate_method == "daily":
                freq = "D"
            elif aggregate_method == "weekly":
                freq = "W"
            elif aggregate_method == "monthly":
                freq = "M"
            else:
                print_warning(f"ä¸æ”¯æŒçš„èšåˆæ–¹å¼: {aggregate_method}ï¼Œè·³è¿‡èšåˆ")
                print(f"  âŒ èšåˆå¤±è´¥: ä¸æ”¯æŒçš„èšåˆæ–¹å¼ '{aggregate_method}'")
                print(f"  ğŸ’¡ æ”¯æŒçš„èšåˆæ–¹å¼: daily, weekly, monthly")
                return data
            
            # é€‰æ‹©èšåˆå­—æ®µ
            aggregate_columns = aggregate_config["èšåˆå­—æ®µ"]
            if aggregate_columns == "auto":
                # è‡ªåŠ¨æ£€æµ‹æ•°å€¼åˆ—
                numeric_columns = data_temp.select_dtypes(include=[np.number]).columns
                exclude_columns = aggregate_config.get("æ’é™¤å­—æ®µ", ['user_id'])
                aggregate_columns = [col for col in numeric_columns if col not in exclude_columns]
                print(f"  ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ° {len(aggregate_columns)} ä¸ªæ•°å€¼å­—æ®µç”¨äºèšåˆ")
            else:
                # ä½¿ç”¨æŒ‡å®šçš„èšåˆå­—æ®µ
                aggregate_columns = [col for col in aggregate_columns if col in data_temp.columns]
                print(f"  ğŸ“‹ ä½¿ç”¨æŒ‡å®šçš„ {len(aggregate_columns)} ä¸ªå­—æ®µè¿›è¡Œèšåˆ")
            
            print_info(f"èšåˆå­—æ®µ: {aggregate_columns}")
            
            if not aggregate_columns:
                print_warning("æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„æ•°å€¼åˆ—")
                print(f"  âŒ èšåˆå¤±è´¥: æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„æ•°å€¼åˆ—")
                print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ•°å€¼å­—æ®µï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šèšåˆå­—æ®µ")
                return data
            
            # æ‰§è¡Œèšåˆ
            print(f"  ğŸš€ å¼€å§‹æ‰§è¡Œèšåˆæ“ä½œ...")
            if aggregate_function == "sum":
                aggregated = data_temp[aggregate_columns].resample(freq).sum()
            elif aggregate_function == "mean":
                aggregated = data_temp[aggregate_columns].resample(freq).mean()
            elif aggregate_function == "median":
                aggregated = data_temp[aggregate_columns].resample(freq).median()
            elif aggregate_function == "max":
                aggregated = data_temp[aggregate_columns].resample(freq).max()
            elif aggregate_function == "min":
                aggregated = data_temp[aggregate_columns].resample(freq).min()
            else:
                print_warning(f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {aggregate_function}ï¼Œä½¿ç”¨sum")
                aggregated = data_temp[aggregate_columns].resample(freq).sum()
            
            # å¤„ç†èšåˆåçš„ç¼ºå¤±å€¼
            if aggregate_config.get("å¤„ç†ç¼ºå¤±å€¼", True):
                missing_fill = aggregate_config.get("ç¼ºå¤±å€¼å¡«å……", 0)
                aggregated = aggregated.fillna(missing_fill)
                print(f"  ğŸ”§ å·²å¤„ç†èšåˆåçš„ç¼ºå¤±å€¼ (å¡«å……å€¼: {missing_fill})")
            
            # é‡ç½®ç´¢å¼•
            if aggregate_config.get("è¾“å‡ºæ ¼å¼", {}).get("é‡ç½®ç´¢å¼•", True):
                aggregated = aggregated.reset_index()
                print(f"  ğŸ”„ å·²é‡ç½®æ—¶é—´ç´¢å¼•")
            
            # è¾“å‡ºè¯¦ç»†èšåˆæ•ˆæœ
            print("\n=================ã€æ•°æ®èšåˆæ•ˆæœã€‘=================")
            print(f"èšåˆæ–¹å¼: {aggregate_method} ({freq})    èšåˆå‡½æ•°: {aggregate_function}")
            print(f"èšåˆå­—æ®µæ•°é‡: {len(aggregate_columns)} ä¸ª")
            print(f"èšåˆå­—æ®µå: {aggregate_columns}")
            print(f"èšåˆå‰æ•°æ®é‡: {len(data):,} æ¡    èšåˆåæ•°æ®é‡: {len(aggregated):,} æ¡")
            print(f"èšåˆæ—¶é—´èŒƒå›´: {aggregated[time_field].min() if time_field in aggregated.columns else aggregated.index.min()} ~ {aggregated[time_field].max() if time_field in aggregated.columns else aggregated.index.max()}")
            print("================================================\n")
            
            print(f"  âœ… èšåˆæ“ä½œå®Œæˆ!")
            return aggregated
        except Exception as e:
            print_error(f"æ•°æ®èšåˆå¤±è´¥: {e}")
            print(f"  âŒ èšåˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®æ ¼å¼ã€æ—¶é—´å­—æ®µæ ¼å¼ã€èšåˆé…ç½®ç­‰")
        return data
    
    def engineer_features(self):
        """
        ç‰¹å¾å·¥ç¨‹
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("ç‰¹å¾å·¥ç¨‹")
        
        if self.data is None:
            print_error("è¯·å…ˆåŠ è½½å’Œæ¸…æ´—æ•°æ®")
            return False
        
        try:
            data = self.data.copy()
            original_columns = len(data.columns)
            
            # 1. è®¡ç®—åŸºç¡€ç‰¹å¾
            if self.feature_config["åŸºç¡€ç‰¹å¾"]["å¯ç”¨"]:
                print_info("è®¡ç®—åŸºç¡€ç‰¹å¾...")
                data = self._calculate_basic_features(data)
            
            # 2. è®¡ç®—æ—¶é—´ç‰¹å¾
            if self.feature_config["æ—¶é—´ç‰¹å¾"]["å¯ç”¨"]:
                print_info("è®¡ç®—æ—¶é—´ç‰¹å¾...")
                data = self._calculate_time_features(data)
            
            # 3. è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            if self.feature_config["ç»Ÿè®¡ç‰¹å¾"]["å¯ç”¨"]:
                print_info("è®¡ç®—ç»Ÿè®¡ç‰¹å¾...")
                data = self._calculate_statistical_features(data)
            
            # 4. è®¡ç®—ç”¨æˆ·ç‰¹å¾
            if self.feature_config["ç”¨æˆ·ç‰¹å¾"]["å¯ç”¨"]:
                print_info("è®¡ç®—ç”¨æˆ·ç‰¹å¾...")
                data = self._calculate_user_features(data)
            
            # 5. è®¡ç®—ä¸šåŠ¡ç‰¹å¾
            if self.feature_config["ä¸šåŠ¡ç‰¹å¾"]["å¯ç”¨"]:
                print_info("è®¡ç®—ä¸šåŠ¡ç‰¹å¾...")
                data = self._calculate_business_features(data)
            
            self.data = data
            new_columns = len(data.columns)
            
            print_success(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {original_columns} -> {new_columns} åˆ—")
            print(f"æ–°å¢ç‰¹å¾: {new_columns - original_columns} ä¸ª")
            
            self.processing_log.append({
                "æ­¥éª¤": "ç‰¹å¾å·¥ç¨‹",
                "åŸå§‹åˆ—æ•°": original_columns,
                "æ–°åˆ—æ•°": new_columns,
                "æ–°å¢ç‰¹å¾æ•°": new_columns - original_columns,
                "æ—¶é—´": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            print_error(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
            return False
    
    def _calculate_basic_features(self, data):
        """è®¡ç®—åŸºç¡€ç‰¹å¾"""
        basic_config = self.feature_config["åŸºç¡€ç‰¹å¾"]
        
        # å‡€èµ„é‡‘æµ
        if basic_config["å‡€èµ„é‡‘æµ"]:
            purchase_field = self.field_mapping["ç”³è´­é‡‘é¢å­—æ®µ"]
            redemption_field = self.field_mapping["èµå›é‡‘é¢å­—æ®µ"]
            
            if purchase_field in data.columns and redemption_field in data.columns:
                data['Net_Flow'] = data[purchase_field] - data[redemption_field]
        
        # æ€»èµ„é‡‘æµ
        if basic_config["æ€»èµ„é‡‘æµ"]:
            purchase_field = self.field_mapping["ç”³è´­é‡‘é¢å­—æ®µ"]
            redemption_field = self.field_mapping["èµå›é‡‘é¢å­—æ®µ"]
            
            if purchase_field in data.columns and redemption_field in data.columns:
                data['Total_Flow'] = data[purchase_field] + data[redemption_field]
        
        # èµ„é‡‘æµæ¯”ç‡
        if basic_config["èµ„é‡‘æµæ¯”ç‡"]:
            purchase_field = self.field_mapping["ç”³è´­é‡‘é¢å­—æ®µ"]
            redemption_field = self.field_mapping["èµå›é‡‘é¢å­—æ®µ"]
            
            if purchase_field in data.columns and redemption_field in data.columns:
                data['Flow_Ratio'] = data[redemption_field] / (data[purchase_field] + 1e-8)  # é¿å…é™¤é›¶
        
        # ä½™é¢å˜åŒ–
        if basic_config["ä½™é¢å˜åŒ–"]:
            current_balance_field = self.field_mapping["å½“å‰ä½™é¢å­—æ®µ"]
            previous_balance_field = self.field_mapping["æ˜¨æ—¥ä½™é¢å­—æ®µ"]
            
            if current_balance_field in data.columns and previous_balance_field in data.columns:
                data['Balance_Change'] = data[current_balance_field] - data[previous_balance_field]
        
        # ä½™é¢å˜åŒ–ç‡
        if basic_config["ä½™é¢å˜åŒ–ç‡"]:
            current_balance_field = self.field_mapping["å½“å‰ä½™é¢å­—æ®µ"]
            previous_balance_field = self.field_mapping["æ˜¨æ—¥ä½™é¢å­—æ®µ"]
            
            if current_balance_field in data.columns and previous_balance_field in data.columns:
                data['Balance_Change_Rate'] = data['Balance_Change'] / (data[previous_balance_field] + 1e-8)
        
        return data
    
    def _calculate_time_features(self, data):
        """è®¡ç®—æ—¶é—´ç‰¹å¾"""
        time_config = self.feature_config["æ—¶é—´ç‰¹å¾"]
        time_field = self.field_mapping["æ—¶é—´å­—æ®µ"]
        
        if time_field not in data.columns:
            return data
        
        # åŸºç¡€æ—¶é—´ç‰¹å¾ï¼ˆå·²åœ¨æ•°æ®æ¸…æ´—é˜¶æ®µæ·»åŠ ï¼‰
        if time_config["å¹´ä¸­å¤©æ•°"]:
            data['DayOfYear'] = data[time_field].dt.dayofyear
        if time_config["å¹´ä¸­å‘¨æ•°"]:
            data['WeekOfYear'] = data[time_field].dt.isocalendar().week
        if time_config["æœˆ"]:
            data['MonthOfYear'] = data[time_field].dt.month
        
        # æœˆåˆæœˆæœ«
        if time_config["æœˆåˆæœˆæœ«"]:
            data['IsMonthStart'] = data[time_field].dt.is_month_start.astype(int)
            data['IsMonthEnd'] = data[time_field].dt.is_month_end.astype(int)
        
        # å‘¨æœ«
        if time_config["å‘¨æœ«"]:
            data['IsWeekend'] = (data['Weekday'] >= 5).astype(int)
        
        # èŠ‚å‡æ—¥ï¼ˆç®€å•åˆ¤æ–­ï¼‰
        if time_config["èŠ‚å‡æ—¥"]:
            data['IsHoliday'] = ((data['Month'] == 1) & (data['Day'] <= 3)).astype(int)  # å…ƒæ—¦
            data['IsHoliday'] |= ((data['Month'] == 5) & (data['Day'] >= 1) & (data['Day'] <= 3)).astype(int)  # åŠ³åŠ¨èŠ‚
            data['IsHoliday'] |= ((data['Month'] == 10) & (data['Day'] >= 1) & (data['Day'] <= 7)).astype(int)  # å›½åº†èŠ‚
        
        return data
    
    def _calculate_statistical_features(self, data):
        """è®¡ç®—ç»Ÿè®¡ç‰¹å¾"""
        stats_config = self.feature_config["ç»Ÿè®¡ç‰¹å¾"]
        feature_list = stats_config["ç‰¹å¾åˆ—è¡¨"]
        windows = stats_config["æ»šåŠ¨çª—å£"]
        functions = stats_config["ç»Ÿè®¡å‡½æ•°"]
        
        # è·å–å¯ç”¨çš„ç‰¹å¾
        available_features = [col for col in feature_list if col in data.columns]
        
        if available_features:
            for feature in available_features:
                for window in windows:
                    for func in functions:
                        if func == "mean":
                            data[f'{feature}_{window}d_mean'] = data[feature].rolling(window=window, min_periods=1).mean()
                        elif func == "std":
                            data[f'{feature}_{window}d_std'] = data[feature].rolling(window=window, min_periods=1).std()
                        elif func == "max":
                            data[f'{feature}_{window}d_max'] = data[feature].rolling(window=window, min_periods=1).max()
                        elif func == "min":
                            data[f'{feature}_{window}d_min'] = data[feature].rolling(window=window, min_periods=1).min()
        
        return data
    
    def _calculate_user_features(self, data):
        """è®¡ç®—ç”¨æˆ·ç‰¹å¾"""
        user_id_field = self.field_mapping["ç”¨æˆ·IDå­—æ®µ"]
        if user_id_field not in data.columns:
            return data
        
        # ç”¨æˆ·çº§åˆ«çš„ç»Ÿè®¡ç‰¹å¾
        user_stats = data.groupby(user_id_field).agg({
            'Net_Flow': ['mean', 'std', 'sum', 'count'],
            'Total_Flow': ['mean', 'std', 'sum'],
            'Balance_Change': ['mean', 'std', 'sum']
        }).reset_index()
        
        # é‡å‘½ååˆ—
        user_stats.columns = [
            user_id_field,
            'User_NetFlow_Mean', 'User_NetFlow_Std', 'User_NetFlow_Sum', 'User_Transaction_Count',
            'User_TotalFlow_Mean', 'User_TotalFlow_Std', 'User_TotalFlow_Sum',
            'User_BalanceChange_Mean', 'User_BalanceChange_Std', 'User_BalanceChange_Sum'
        ]
        
        # åˆå¹¶å›åŸæ•°æ®
        data = data.merge(user_stats, on=user_id_field, how='left')
        
        return data
    
    def _calculate_business_features(self, data):
        """è®¡ç®—ä¸šåŠ¡ç‰¹å¾"""
        # äº¤æ˜“æ´»è·ƒåº¦
        if 'User_Transaction_Count' in data.columns:
            data['Transaction_Activity'] = pd.cut(
                data['User_Transaction_Count'],
                bins=[0, 10, 50, 100, float('inf')],
                labels=['Low', 'Medium', 'High', 'Very_High']
            )
        
        # èµ„é‡‘æµç±»å‹
        if 'Net_Flow' in data.columns:
            data['Flow_Type'] = pd.cut(
                data['Net_Flow'],
                bins=[float('-inf'), -1000, 0, 1000, float('inf')],
                labels=['Large_Outflow', 'Outflow', 'Inflow', 'Large_Inflow']
            )
        
        # ä½™é¢æ°´å¹³
        current_balance_field = self.field_mapping["å½“å‰ä½™é¢å­—æ®µ"]
        if current_balance_field in data.columns:
            data['Balance_Level'] = pd.cut(
                data[current_balance_field],
                bins=[0, 10000, 100000, 1000000, float('inf')],
                labels=['Low', 'Medium', 'High', 'Very_High']
            )
        
        return data
    
    def transform_data(self):
        """
        æ•°æ®è½¬æ¢
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("æ•°æ®è½¬æ¢")
        
        if self.data is None:
            print_error("è¯·å…ˆå®Œæˆç‰¹å¾å·¥ç¨‹")
            return False
        
        try:
            data = self.data.copy()
            
            # 1. æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
            if self.transformation_config["æ ‡å‡†åŒ–"]["å¯ç”¨"]:
                print_info("æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾...")
                data = self._standardize_numeric_features(data)
            
            # 2. ç¼–ç åˆ†ç±»ç‰¹å¾
            if self.transformation_config["ç¼–ç "]["å¯ç”¨"]:
                print_info("ç¼–ç åˆ†ç±»ç‰¹å¾...")
                data = self._encode_categorical_features(data)
            
            # 3. å¤„ç†æ—¶é—´åºåˆ—ç‰¹å¾
            if self.transformation_config["æ—¶é—´åºåˆ—ç‰¹å¾"]["å¯ç”¨"]:
                print_info("å¤„ç†æ—¶é—´åºåˆ—ç‰¹å¾...")
                data = self._process_time_series_features(data)
            
            # 4. ç‰¹å¾é€‰æ‹©
            if self.transformation_config["ç‰¹å¾é€‰æ‹©"]["å¯ç”¨"]:
                print_info("ç‰¹å¾é€‰æ‹©...")
                data = self._select_features(data)
            
            self.processed_data = data
            
            print_success(f"æ•°æ®è½¬æ¢å®Œæˆ: {len(data.columns)} ä¸ªç‰¹å¾")
            
            self.processing_log.append({
                "æ­¥éª¤": "æ•°æ®è½¬æ¢",
                "æœ€ç»ˆç‰¹å¾æ•°": len(data.columns),
                "æ—¶é—´": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            print_error(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return False
    
    def _standardize_numeric_features(self, data):
        """æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾"""
        standardize_config = self.transformation_config["æ ‡å‡†åŒ–"]
        feature_list = standardize_config["ç‰¹å¾åˆ—è¡¨"]
        method = standardize_config["æ–¹æ³•"]
        
        available_features = [col for col in feature_list if col in data.columns]
        
        for feature in available_features:
            if method == "zscore":
                mean_val = data[feature].mean()
                std_val = data[feature].std()
                if std_val > 0:
                    data[f'{feature}_Normalized'] = (data[feature] - mean_val) / std_val
            elif method == "minmax":
                min_val = data[feature].min()
                max_val = data[feature].max()
                if max_val > min_val:
                    data[f'{feature}_Normalized'] = (data[feature] - min_val) / (max_val - min_val)
            elif method == "robust":
                median_val = data[feature].median()
                q75 = data[feature].quantile(0.75)
                q25 = data[feature].quantile(0.25)
                iqr = q75 - q25
                if iqr > 0:
                    data[f'{feature}_Normalized'] = (data[feature] - median_val) / iqr
        
        return data
    
    def _encode_categorical_features(self, data):
        """ç¼–ç åˆ†ç±»ç‰¹å¾"""
        encode_config = self.transformation_config["ç¼–ç "]
        feature_list = encode_config["ç‰¹å¾åˆ—è¡¨"]
        method = encode_config["æ–¹æ³•"]
        
        for feature in feature_list:
            if feature in data.columns:
                if method == "onehot":
                    # ç‹¬çƒ­ç¼–ç 
                    dummies = pd.get_dummies(data[feature], prefix=feature)
                    data = pd.concat([data, dummies], axis=1)
                    data = data.drop(columns=[feature])
                elif method == "label":
                    # æ ‡ç­¾ç¼–ç 
                    from sklearn.preprocessing import LabelEncoder
                    le = LabelEncoder()
                    data[f'{feature}_Encoded'] = le.fit_transform(data[feature].astype(str))
                    data = data.drop(columns=[feature])
        
        return data
    
    def _process_time_series_features(self, data):
        """å¤„ç†æ—¶é—´åºåˆ—ç‰¹å¾"""
        ts_config = self.transformation_config["æ—¶é—´åºåˆ—ç‰¹å¾"]
        feature_list = ts_config["ç‰¹å¾åˆ—è¡¨"]
        lag_periods = ts_config["æ»åæœŸæ•°"]
        
        if ts_config["æ»åç‰¹å¾"]:
            available_features = [col for col in feature_list if col in data.columns]
            
            for feature in available_features:
                for lag in lag_periods:
                    data[f'{feature}_Lag{lag}'] = data[feature].shift(lag)
        
        return data
    
    def _select_features(self, data):
        """ç‰¹å¾é€‰æ‹©"""
        selection_config = self.transformation_config["ç‰¹å¾é€‰æ‹©"]
        missing_threshold = selection_config["ç¼ºå¤±å€¼é˜ˆå€¼"]
        correlation_threshold = selection_config["ç›¸å…³æ€§é˜ˆå€¼"]
        variance_threshold = selection_config["æ–¹å·®é˜ˆå€¼"]
        
        # ç§»é™¤åŒ…å«å¤ªå¤šç¼ºå¤±å€¼çš„ç‰¹å¾
        missing_ratio = data.isnull().sum() / len(data)
        columns_to_drop = missing_ratio[missing_ratio > missing_threshold].index
        data = data.drop(columns=columns_to_drop)
        
        # ç§»é™¤å¸¸é‡ç‰¹å¾
        constant_columns = [col for col in data.columns if data[col].nunique() <= 1]
        data = data.drop(columns=constant_columns)
        
        # ç§»é™¤ä½æ–¹å·®ç‰¹å¾
        numeric_data = data.select_dtypes(include=[np.number])
        variance = numeric_data.var()
        low_variance_columns = variance[variance < variance_threshold].index
        data = data.drop(columns=low_variance_columns)
        
        # ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾
        correlation_matrix = numeric_data.corr().abs()
        upper_triangle = correlation_matrix.where(
            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
        )
        high_corr_columns = [column for column in upper_triangle.columns 
                           if any(upper_triangle[column] > correlation_threshold)]
        data = data.drop(columns=high_corr_columns)
        
        return data
    
    def save_processed_data(self, output_path=None):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
        """
        if self.processed_data is None:
            print_error("æ²¡æœ‰å¤„ç†åçš„æ•°æ®å¯ä¿å­˜")
            return False
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = OUTPUT_DATA_DIR / f"processed_data_{timestamp}.csv"
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ•°æ®
            self.processed_data.to_csv(output_path, index=False, encoding='utf-8')
            print_success(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {output_path}")
            
            # ä¿å­˜å¤„ç†æ—¥å¿—
            log_path = output_path.parent / f"processing_log_{timestamp}.json"
            write_json(self.processing_log, log_path)
            print_info(f"å¤„ç†æ—¥å¿—å·²ä¿å­˜: {log_path}")
            
            return True
            
        except Exception as e:
            print_error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def get_processed_data_path(self):
        """
        è·å–å¤„ç†åçš„æ•°æ®è·¯å¾„
        
        Returns:
            str: å¤„ç†åæ•°æ®è·¯å¾„
        """
        if self.processed_data is None:
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„å¤„ç†åæ•°æ®æ–‡ä»¶
        output_dir = OUTPUT_DATA_DIR / "data"
        if output_dir.exists():
            processed_files = list(output_dir.glob("processed_data_*.csv"))
            if processed_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
                latest_file = max(processed_files, key=lambda x: x.stat().st_mtime)
                return str(latest_file)
        
        return None
    
    def run_full_pipeline(self, file_path=None):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print_header("å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿", "æ•°æ®æ¸…æ´— -> ç‰¹å¾å·¥ç¨‹ -> æ•°æ®è½¬æ¢")
        
        # 1. åŠ è½½å’Œåˆ†ææ•°æ®
        if not self.load_and_analyze_data(file_path):
            return False
        
        # 2. å°è¯•åŠ è½½ç°æœ‰é…ç½®
        data_source_name = self.data_file_path.stem if hasattr(self, 'data_file_path') else None
        config_loaded = self.load_processing_config(data_source_name)
        
        if config_loaded:
            print_info("ä½¿ç”¨ç°æœ‰é…ç½®æ–‡ä»¶è¿›è¡Œå¤„ç†")
        else:
            print_info("ä½¿ç”¨é»˜è®¤é…ç½®è¿›è¡Œå¤„ç†")
        
        # 3. æ•°æ®æ¸…æ´—
        if not self.clean_data():
            return False
        
        # 4. ç‰¹å¾å·¥ç¨‹
        if not self.engineer_features():
            return False
        
        # 5. æ•°æ®è½¬æ¢
        if not self.transform_data():
            return False
        
        # 6. ä¿å­˜ç»“æœ
        if not self.save_processed_data():
            return False
        
        # 7. ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = self.save_processing_config(data_source_name)
        
        # 8. ç”Ÿæˆè¯´æ˜æ–‡æ¡£
        self.generate_data_folder_readme(data_source_name)
        
        print_success("å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        return True

    def save_processing_config(self, data_source_name=None):
        """
        ä¿å­˜æ•°æ®å¤„ç†é…ç½®åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ç›®å½•
        
        Args:
            data_source_name: æ•°æ®æºåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if data_source_name is None:
            # ä»æ•°æ®æ–‡ä»¶åç”Ÿæˆæ•°æ®æºåç§°
            if hasattr(self, 'data_file_path') and self.data_file_path:
                data_source_name = self.data_file_path.stem
            else:
                data_source_name = "user_balance_table"
        
        config_file_path = DATA_DIR / f"{data_source_name}_preprocessing_config.json"
        
        try:
            # ç”Ÿæˆé…ç½®ä¿¡æ¯
            config = {
                "data_source_name": data_source_name,
                "generated_time": datetime.now().isoformat(),
                "data_info": {
                    "original_shape": self.data.shape if self.data is not None else None,
                    "processed_shape": self.processed_data.shape if self.processed_data is not None else None,
                    "feature_count": len(self.processed_data.columns) if self.processed_data is not None else None
                },
                "preprocessing_config": {
                    "ç¼ºå¤±å€¼å¤„ç†": self.preprocessing_config["ç¼ºå¤±å€¼å¤„ç†"],
                    "å¼‚å¸¸å€¼å¤„ç†": self.preprocessing_config["å¼‚å¸¸å€¼å¤„ç†"],
                    "æ—¶é—´ç‰¹å¾": self.preprocessing_config["æ—¶é—´ç‰¹å¾"],
                    "æ•°æ®èšåˆ": self.preprocessing_config["æ•°æ®èšåˆ"],
                    "æ•°æ®ä¸€è‡´æ€§å¤„ç†": self.preprocessing_config["æ•°æ®ä¸€è‡´æ€§å¤„ç†"]
                },
                "feature_engineering_config": {
                    "åŸºç¡€ç‰¹å¾": self.feature_config["åŸºç¡€ç‰¹å¾"],
                    "æ—¶é—´ç‰¹å¾": self.feature_config["æ—¶é—´ç‰¹å¾"],
                    "ç»Ÿè®¡ç‰¹å¾": self.feature_config["ç»Ÿè®¡ç‰¹å¾"],
                    "ç”¨æˆ·ç‰¹å¾": self.feature_config["ç”¨æˆ·ç‰¹å¾"],
                    "ä¸šåŠ¡ç‰¹å¾": self.feature_config["ä¸šåŠ¡ç‰¹å¾"]
                },
                "data_transformation_config": {
                    "æ ‡å‡†åŒ–": self.transformation_config["æ ‡å‡†åŒ–"],
                    "ç¼–ç ": self.transformation_config["ç¼–ç "],
                    "æ—¶é—´åºåˆ—ç‰¹å¾": self.transformation_config["æ—¶é—´åºåˆ—ç‰¹å¾"],
                    "ç‰¹å¾é€‰æ‹©": self.transformation_config["ç‰¹å¾é€‰æ‹©"]
                },
                "field_mapping": self.field_mapping,
                "processing_log": self.processing_log
            }
            
            # ä¿å­˜é…ç½®æ–‡ä»¶
            write_json(config, config_file_path)
            print_success(f"æ•°æ®å¤„ç†é…ç½®å·²ä¿å­˜: {config_file_path}")
            
            return config_file_path
            
        except Exception as e:
            print_error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def load_processing_config(self, data_source_name=None):
        """
        ä»åŸå§‹æ•°æ®æ–‡ä»¶ç›®å½•åŠ è½½æ•°æ®å¤„ç†é…ç½®
        
        Args:
            data_source_name: æ•°æ®æºåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½é…ç½®
        """
        if data_source_name is None:
            # è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶
            config_files = list(DATA_DIR.glob("*_preprocessing_config.json"))
            if not config_files:
                print_info("æœªæ‰¾åˆ°é¢„å¤„ç†é…ç½®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
                return False
            
            # ä½¿ç”¨æœ€æ–°çš„é…ç½®æ–‡ä»¶
            config_file_path = max(config_files, key=lambda x: x.stat().st_mtime)
            data_source_name = config_file_path.stem.replace("_preprocessing_config", "")
        else:
            config_file_path = DATA_DIR / f"{data_source_name}_preprocessing_config.json"
        
        if not config_file_path.exists():
            print_info(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_file_path}")
            return False
        
        try:
            # åŠ è½½é…ç½®æ–‡ä»¶
            with open(config_file_path, 'r', encoding='utf-8') as f:
                import json
                config = json.load(f)
            
            # æ›´æ–°é…ç½®
            if "preprocessing_config" in config:
                self.preprocessing_config.update(config["preprocessing_config"])
            
            if "feature_engineering_config" in config:
                self.feature_config.update(config["feature_engineering_config"])
            
            if "data_transformation_config" in config:
                self.transformation_config.update(config["data_transformation_config"])
            
            if "field_mapping" in config:
                self.field_mapping.update(config["field_mapping"])
            
            print_success(f"å·²åŠ è½½é¢„å¤„ç†é…ç½®: {config_file_path}")
            print_info(f"æ•°æ®æº: {config.get('data_source_name', 'unknown')}")
            print_info(f"ç”Ÿæˆæ—¶é—´: {config.get('generated_time', 'unknown')}")
            
            return True
            
        except Exception as e:
            print_error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def generate_data_folder_readme(self, data_source_name=None):
        """
        ç”Ÿæˆdataæ–‡ä»¶å¤¹è¯´æ˜æ–‡æ¡£
        
        Args:
            data_source_name: æ•°æ®æºåç§°
        """
        if data_source_name is None:
            data_source_name = "user_balance_table"
        
        readme_path = DATA_DIR / "README.md"
        
        try:
            # è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨
            data_files = list(DATA_DIR.glob("*.csv"))
            config_files = list(DATA_DIR.glob("*_preprocessing_config.json"))
            
            # ç”ŸæˆREADMEå†…å®¹
            readme_content = f"""# Data æ–‡ä»¶å¤¹è¯´æ˜

## æ¦‚è¿°
dataæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾åŸå§‹æ•°æ®æ–‡ä»¶å’Œç›¸å…³çš„é…ç½®æ–‡ä»¶ã€‚

## æ–‡ä»¶ç»“æ„

### åŸå§‹æ•°æ®æ–‡ä»¶
"""
            
            for data_file in data_files:
                file_size = data_file.stat().st_size / (1024 * 1024)  # MB
                readme_content += f"- **{data_file.name}**: {file_size:.2f} MB\n"
            
            readme_content += """
### é…ç½®æ–‡ä»¶
"""
            
            for config_file in config_files:
                readme_content += f"- **{config_file.name}**: æ•°æ®å¤„ç†é…ç½®æ–‡ä»¶\n"
            
            readme_content += f"""
## æ•°æ®é¢„å¤„ç†é…ç½®

### å½“å‰æ•°æ®æº: {data_source_name}

ç³»ç»Ÿæ”¯æŒè‡ªåŠ¨ç”Ÿæˆå’ŒåŠ è½½æ•°æ®å¤„ç†é…ç½®ï¼Œé…ç½®æ–‡ä»¶åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

#### 1. æ•°æ®é¢„å¤„ç†é…ç½®
- **ç¼ºå¤±å€¼å¤„ç†**: å®šä¹‰å„å­—æ®µçš„ç¼ºå¤±å€¼å¡«å……ç­–ç•¥
- **å¼‚å¸¸å€¼å¤„ç†**: å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†æ–¹æ³•
- **æ—¶é—´ç‰¹å¾**: æ—¶é—´ç‰¹å¾æå–é…ç½®

#### 2. ç‰¹å¾å·¥ç¨‹é…ç½®
- **åŸºç¡€ç‰¹å¾**: å‡€èµ„é‡‘æµã€æ€»èµ„é‡‘æµã€èµ„é‡‘æµæ¯”ç‡ç­‰
- **æ—¶é—´ç‰¹å¾**: å¹´ã€æœˆã€æ—¥ã€æ˜ŸæœŸã€å­£åº¦ç­‰æ—¶é—´ç‰¹å¾
- **ç»Ÿè®¡ç‰¹å¾**: æ»šåŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
- **ç”¨æˆ·ç‰¹å¾**: ç”¨æˆ·çº§åˆ«ç»Ÿè®¡å’Œåˆ†ç±»ç‰¹å¾
- **ä¸šåŠ¡ç‰¹å¾**: äº¤æ˜“æ´»è·ƒåº¦ã€èµ„é‡‘æµç±»å‹ç­‰

#### 3. æ•°æ®è½¬æ¢é…ç½®
- **æ ‡å‡†åŒ–**: æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–æ–¹æ³•
- **ç¼–ç **: åˆ†ç±»ç‰¹å¾ç¼–ç æ–¹æ³•
- **æ—¶é—´åºåˆ—ç‰¹å¾**: æ»åç‰¹å¾é…ç½®
- **ç‰¹å¾é€‰æ‹©**: ç‰¹å¾ç­›é€‰ç­–ç•¥

## ä½¿ç”¨æ–¹æ³•

### 1. é¦–æ¬¡å¤„ç†æ•°æ®
1. å°†åŸå§‹æ•°æ®æ–‡ä»¶æ”¾å…¥dataæ–‡ä»¶å¤¹
2. è¿è¡Œæ•°æ®é¢„å¤„ç†åŠŸèƒ½
3. ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆé…ç½®æ–‡ä»¶

### 2. ä¿®æ”¹å¤„ç†å‚æ•°
1. ç¼–è¾‘å¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼ˆ*_preprocessing_config.jsonï¼‰
2. é‡æ–°è¿è¡Œæ•°æ®é¢„å¤„ç†åŠŸèƒ½
3. ç³»ç»Ÿå°†ä½¿ç”¨ä¿®æ”¹åçš„é…ç½®

### 3. é…ç½®æ–‡ä»¶æ ¼å¼
é…ç½®æ–‡ä»¶é‡‡ç”¨JSONæ ¼å¼ï¼ŒåŒ…å«è¯¦ç»†çš„å¤„ç†å‚æ•°å’Œè¯´æ˜ã€‚

## æ³¨æ„äº‹é¡¹

1. é…ç½®æ–‡ä»¶ä¸åŸå§‹æ•°æ®æ–‡ä»¶å…³è”ï¼Œä¿®æ”¹æ•°æ®æ–‡ä»¶åéœ€è¦é‡æ–°ç”Ÿæˆé…ç½®
2. å»ºè®®åœ¨ä¿®æ”¹é…ç½®å‰å¤‡ä»½åŸé…ç½®æ–‡ä»¶
3. é…ç½®æ–‡ä»¶åŒ…å«å¤„ç†æ—¥å¿—ï¼Œä¾¿äºè¿½è¸ªå¤„ç†è¿‡ç¨‹
4. æ‰€æœ‰é…ç½®æ–‡ä»¶ä½¿ç”¨UTF-8ç¼–ç 

## æ–‡ä»¶å‘½åè§„åˆ™

- **åŸå§‹æ•°æ®**: `{data_source_name}.csv`
- **é¢„å¤„ç†é…ç½®**: `{data_source_name}_preprocessing_config.json`
- **æ•°æ®æºé…ç½®**: `{data_source_name}_dispose.json`ï¼ˆç”±åŸºç¡€æ•°æ®åˆ†æç”Ÿæˆï¼‰

## æ›´æ–°è®°å½•

- é…ç½®æ–‡ä»¶ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- æœ€åæ›´æ–°: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
            
            # ä¿å­˜READMEæ–‡ä»¶
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            
            print_success(f"Dataæ–‡ä»¶å¤¹è¯´æ˜æ–‡æ¡£å·²ç”Ÿæˆ: {readme_path}")
            return True
            
        except Exception as e:
            print_error(f"ç”Ÿæˆè¯´æ˜æ–‡æ¡£å¤±è´¥: {e}")
            return False


def run_data_processing():
    """è¿è¡Œæ•°æ®å¤„ç†åŠŸèƒ½"""
    print_header("æ•°æ®å¤„ç†æ¨¡å—", "æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®è½¬æ¢")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹åŒ–é…ç½®çš„æ•°æ®æº
    data_files = list(DATA_DIR.glob("*/*.csv"))
    if data_files:
        print_info("æ£€æµ‹åˆ°ç‰¹åŒ–é…ç½®çš„æ•°æ®æºï¼Œä½¿ç”¨é€šç”¨æ•°æ®å¤„ç†å™¨")
        from src.data_processing_universal import run_universal_data_processing
        success = run_universal_data_processing()
    else:
        print_info("æœªæ£€æµ‹åˆ°ç‰¹åŒ–é…ç½®ï¼Œä½¿ç”¨ä¼ ç»Ÿæ•°æ®å¤„ç†å™¨")
        # åˆ›å»ºæ•°æ®å¤„ç†æµæ°´çº¿
        pipeline = DataProcessingPipeline()
        
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        success = pipeline.run_full_pipeline()
        
        if success:
            print_success("æ•°æ®å¤„ç†å®Œæˆï¼")
            print(f"å¤„ç†æ—¥å¿—åŒ…å« {len(pipeline.processing_log)} ä¸ªæ­¥éª¤")
            print("å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•")
        else:
            print_error("æ•°æ®å¤„ç†å¤±è´¥ï¼")
    
    return success


if __name__ == "__main__":
    run_data_processing() 