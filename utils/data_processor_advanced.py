# -*- coding: utf-8 -*-
"""
é«˜çº§æ•°æ®å¤„ç†æ¨¡å—
ç”¨äºå¤„ç†å¤§æ•°æ®é‡ï¼ŒåŒ…æ‹¬é‡‡æ ·ã€è¿‡æ»¤ã€èšåˆç­‰åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from config import DATA_PROCESSING_CONFIG, OUTPUT_DATA_DIR
from utils.interactive_utils import print_header, print_success, print_error, print_info, print_warning
from utils.file_utils import write_csv, write_json


class AdvancedDataProcessor:
    """é«˜çº§æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            config: å¤„ç†é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or DATA_PROCESSING_CONFIG
        self.original_data = None
        self.processed_data = None
        self.processing_stats = {}
    
    def load_data(self, file_path):
        """
        åŠ è½½æ•°æ®
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            print_header("æ•°æ®åŠ è½½")
            print(f"åŠ è½½æ–‡ä»¶: {file_path}")
            
            self.original_data = pd.read_csv(file_path)
            print_success(f"æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŸå§‹æ•°æ®é‡: {len(self.original_data):,} æ¡")
            print(f"æ•°æ®å½¢çŠ¶: {self.original_data.shape}")
            print(f"åˆ—å: {list(self.original_data.columns)}")
            
            return True
        except Exception as e:
            print_error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def time_range_sampling(self, data):
        """
        æ—¶é—´èŒƒå›´é‡‡æ ·
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: é‡‡æ ·åçš„æ•°æ®
        """
        if not self.config["æ•°æ®é‡‡æ ·"]["æ—¶é—´èŒƒå›´é‡‡æ ·"]["å¯ç”¨"]:
            return data
        
        time_config = self.config["æ•°æ®é‡‡æ ·"]["æ—¶é—´èŒƒå›´é‡‡æ ·"]
        time_field = self.config["æ•°æ®èšåˆ"]["æ—¶é—´å­—æ®µ"]
        
        if time_field not in data.columns:
            print_error(f"æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨")
            return data
        
        try:
            # è½¬æ¢æ—¶é—´å­—æ®µ
            data[time_field] = pd.to_datetime(data[time_field])
            
            # è®¾ç½®æ—¶é—´èŒƒå›´
            start_date = pd.to_datetime(time_config["å¼€å§‹æ—¥æœŸ"])
            end_date = pd.to_datetime(time_config["ç»“æŸæ—¥æœŸ"])
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´
            filtered_data = data[(data[time_field] >= start_date) & 
                                (data[time_field] <= end_date)]
            
            print_info(f"æ—¶é—´èŒƒå›´é‡‡æ ·: {len(data):,} -> {len(filtered_data):,} æ¡")
            print(f"æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            
            return filtered_data
        except Exception as e:
            print_error(f"æ—¶é—´èŒƒå›´é‡‡æ ·å¤±è´¥: {e}")
            return data
    
    def random_sampling(self, data):
        """
        éšæœºé‡‡æ ·
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: é‡‡æ ·åçš„æ•°æ®
        """
        if not self.config["æ•°æ®é‡‡æ ·"]["å¯ç”¨é‡‡æ ·"]:
            return data
        
        sample_config = self.config["æ•°æ®é‡‡æ ·"]
        sample_ratio = sample_config["é‡‡æ ·æ¯”ä¾‹"]
        max_samples = sample_config["æœ€å¤§æ•°æ®é‡"]
        
        # è®¡ç®—é‡‡æ ·æ•°é‡
        target_samples = min(int(len(data) * sample_ratio), max_samples)
        
        if target_samples >= len(data):
            return data
        
        # éšæœºé‡‡æ ·
        sampled_data = data.sample(n=target_samples, random_state=42)
        
        print_info(f"éšæœºé‡‡æ ·: {len(data):,} -> {len(sampled_data):,} æ¡")
        print(f"é‡‡æ ·æ¯”ä¾‹: {sample_ratio:.1%}")
        
        return sampled_data
    
    def systematic_sampling(self, data):
        """
        ç³»ç»Ÿé‡‡æ ·
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: é‡‡æ ·åçš„æ•°æ®
        """
        if not self.config["æ•°æ®é‡‡æ ·"]["å¯ç”¨é‡‡æ ·"]:
            return data
        
        sample_config = self.config["æ•°æ®é‡‡æ ·"]
        sample_ratio = sample_config["é‡‡æ ·æ¯”ä¾‹"]
        max_samples = sample_config["æœ€å¤§æ•°æ®é‡"]
        
        # è®¡ç®—é‡‡æ ·é—´éš”
        target_samples = min(int(len(data) * sample_ratio), max_samples)
        step = len(data) // target_samples
        
        if step <= 1:
            return data
        
        # ç³»ç»Ÿé‡‡æ ·
        sampled_data = data.iloc[::step].head(target_samples)
        
        print_info(f"ç³»ç»Ÿé‡‡æ ·: {len(data):,} -> {len(sampled_data):,} æ¡")
        print(f"é‡‡æ ·é—´éš”: {step}")
        
        return sampled_data
    
    def detect_outliers(self, data, column):
        """
        æ£€æµ‹å¼‚å¸¸å€¼
        
        Args:
            data: æ•°æ®
            column: åˆ—å
            
        Returns:
            pd.Series: å¼‚å¸¸å€¼æ©ç 
        """
        outlier_config = self.config["æ•°æ®è¿‡æ»¤"]["å¼‚å¸¸å€¼å¤„ç†"]
        method = outlier_config["æ–¹æ³•"]
        threshold = outlier_config["é˜ˆå€¼"]
        
        if method == "iqr":
            Q1 = data[column].quantile(0.25)
            Q3 = data[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            return (data[column] < lower_bound) | (data[column] > upper_bound)
        
        elif method == "zscore":
            z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())
            return z_scores > threshold
        
        elif method == "percentile":
            lower_bound = data[column].quantile(threshold / 100)
            upper_bound = data[column].quantile(1 - threshold / 100)
            return (data[column] < lower_bound) | (data[column] > upper_bound)
        
        return pd.Series([False] * len(data))
    
    def handle_outliers(self, data):
        """
        å¤„ç†å¼‚å¸¸å€¼
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        if not self.config["æ•°æ®è¿‡æ»¤"]["å¼‚å¸¸å€¼å¤„ç†"]["å¯ç”¨"]:
            return data
        
        outlier_config = self.config["æ•°æ®è¿‡æ»¤"]["å¼‚å¸¸å€¼å¤„ç†"]
        method = outlier_config["å¤„ç†æ–¹å¼"]
        
        # æ•°å€¼åˆ—
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        
        total_outliers = 0
        for column in numeric_columns:
            outlier_mask = self.detect_outliers(data, column)
            outlier_count = outlier_mask.sum()
            total_outliers += outlier_count
            
            if outlier_count > 0:
                if method == "remove":
                    data = data[~outlier_mask]
                elif method == "clip":
                    Q1 = data[column].quantile(0.25)
                    Q3 = data[column].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - outlier_config["é˜ˆå€¼"] * IQR
                    upper_bound = Q3 + outlier_config["é˜ˆå€¼"] * IQR
                    data[column] = data[column].clip(lower_bound, upper_bound)
                elif method == "fill":
                    median_val = data[column].median()
                    data.loc[outlier_mask, column] = median_val
        
        if total_outliers > 0:
            print_info(f"å¼‚å¸¸å€¼å¤„ç†: å¤„ç†äº† {total_outliers} ä¸ªå¼‚å¸¸å€¼")
        
        return data
    
    def handle_missing_values(self, data):
        """
        å¤„ç†ç¼ºå¤±å€¼
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        if not self.config["æ•°æ®è¿‡æ»¤"]["ç¼ºå¤±å€¼å¤„ç†"]["å¯ç”¨"]:
            return data
        
        missing_config = self.config["æ•°æ®è¿‡æ»¤"]["ç¼ºå¤±å€¼å¤„ç†"]
        method = missing_config["æ–¹æ³•"]
        fill_value = missing_config["å¡«å……å€¼"]
        max_missing_ratio = missing_config["æœ€å¤§ç¼ºå¤±æ¯”ä¾‹"]
        
        # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
        missing_ratio = data.isnull().sum() / len(data)
        high_missing_columns = missing_ratio[missing_ratio > max_missing_ratio].index
        
        if len(high_missing_columns) > 0:
            print_info(f"åˆ é™¤é«˜ç¼ºå¤±å€¼åˆ—: {list(high_missing_columns)}")
            data = data.drop(columns=high_missing_columns)
        
        # å¤„ç†å‰©ä½™ç¼ºå¤±å€¼
        if method == "drop":
            data = data.dropna()
        elif method == "fill":
            data = data.fillna(fill_value)
        elif method == "interpolate":
            data = data.interpolate()
        
        print_info(f"ç¼ºå¤±å€¼å¤„ç†: å¤„ç†åæ•°æ®é‡ {len(data):,} æ¡")
        
        return data
    
    def remove_duplicates(self, data):
        """
        å¤„ç†é‡å¤å€¼
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        if not self.config["æ•°æ®è´¨é‡"]["é‡å¤å€¼å¤„ç†"]["å¯ç”¨"]:
            return data
        
        duplicate_config = self.config["æ•°æ®è´¨é‡"]["é‡å¤å€¼å¤„ç†"]
        method = duplicate_config["æ–¹æ³•"]
        
        original_count = len(data)
        
        if method == "drop":
            data = data.drop_duplicates()
        elif method == "keep_first":
            data = data.drop_duplicates(keep='first')
        elif method == "keep_last":
            data = data.drop_duplicates(keep='last')
        
        removed_count = original_count - len(data)
        if removed_count > 0:
            print_info(f"é‡å¤å€¼å¤„ç†: åˆ é™¤äº† {removed_count} æ¡é‡å¤è®°å½•")
        
        return data
    
    def aggregate_data(self, data):
        """
        æ•°æ®èšåˆ
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            pd.DataFrame: èšåˆåçš„æ•°æ®
        """
        if not self.config["æ•°æ®èšåˆ"]["å¯ç”¨èšåˆ"]:
            print_info("è·³è¿‡æ•°æ®èšåˆ (èšåˆåŠŸèƒ½æœªå¯ç”¨)")
            print(f"  â­ï¸  èšåˆåŠŸèƒ½å·²ç¦ç”¨ï¼Œä¿æŒåŸå§‹æ•°æ®é‡: {len(data):,} æ¡")
            return data
        
        print_info("å¼€å§‹æ‰§è¡Œæ•°æ®èšåˆ...")
        agg_config = self.config["æ•°æ®èšåˆ"]
        time_field = agg_config["æ—¶é—´å­—æ®µ"]
        agg_way = agg_config["èšåˆæ–¹å¼"]
        agg_func = agg_config["èšåˆå‡½æ•°"]
        
        print(f"  ğŸ”„ èšåˆåŠŸèƒ½å·²å¯ç”¨")
        print(f"  ğŸ“… èšåˆæ–¹å¼: {agg_way}")
        print(f"  ğŸ§® èšåˆå‡½æ•°: {agg_func}")
        print(f"  ğŸ•’ æ—¶é—´å­—æ®µ: {time_field}")
        
        if time_field not in data.columns:
            print_error(f"æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨")
            print(f"  âŒ èšåˆå¤±è´¥: æ—¶é—´å­—æ®µ '{time_field}' ä¸å­˜åœ¨")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ—¶é—´å­—æ®µåç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ 'auto' è‡ªåŠ¨æ£€æµ‹")
            return data
        
        try:
            # ç¡®ä¿æ—¶é—´å­—æ®µæ˜¯datetimeç±»å‹
            data[time_field] = pd.to_datetime(data[time_field])
            print(f"  ğŸ”„ å·²è½¬æ¢æ—¶é—´å­—æ®µ '{time_field}' ä¸ºdatetimeç±»å‹")
            
            # è®¾ç½®æ—¶é—´ç´¢å¼•
            data = data.set_index(time_field)
            
            # æ ¹æ®èšåˆæ–¹å¼è®¾ç½®é‡é‡‡æ ·é¢‘ç‡
            if agg_way == "daily":
                freq = "D"
            elif agg_way == "weekly":
                freq = "W"
            elif agg_way == "monthly":
                freq = "M"
            else:
                print_warning(f"ä¸æ”¯æŒçš„èšåˆæ–¹å¼: {agg_way}")
                print(f"  âŒ èšåˆå¤±è´¥: ä¸æ”¯æŒçš„èšåˆæ–¹å¼ '{agg_way}'")
                print(f"  ğŸ’¡ æ”¯æŒçš„èšåˆæ–¹å¼: daily, weekly, monthly")
                return data
            
            # æ•°å€¼åˆ—
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            print(f"  ğŸ” æ£€æµ‹åˆ° {len(numeric_columns)} ä¸ªæ•°å€¼å­—æ®µç”¨äºèšåˆ")
            
            if len(numeric_columns) == 0:
                print_warning("æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„æ•°å€¼åˆ—")
                print(f"  âŒ èšåˆå¤±è´¥: æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„æ•°å€¼åˆ—")
                print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ•°å€¼å­—æ®µ")
                return data
            
            # èšåˆ
            print(f"  ğŸš€ å¼€å§‹æ‰§è¡Œèšåˆæ“ä½œ...")
            if agg_func == "sum":
                aggregated = data[numeric_columns].resample(freq).sum()
            elif agg_func == "mean":
                aggregated = data[numeric_columns].resample(freq).mean()
            elif agg_func == "median":
                aggregated = data[numeric_columns].resample(freq).median()
            elif agg_func == "max":
                aggregated = data[numeric_columns].resample(freq).max()
            elif agg_func == "min":
                aggregated = data[numeric_columns].resample(freq).min()
            else:
                print_warning(f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {agg_func}ï¼Œä½¿ç”¨sum")
                aggregated = data[numeric_columns].resample(freq).sum()
            
            # é‡ç½®ç´¢å¼•
            aggregated = aggregated.reset_index()
            print(f"  ğŸ”„ å·²é‡ç½®æ—¶é—´ç´¢å¼•")
            
            print_info(f"æ•°æ®èšåˆ: {len(data):,} -> {len(aggregated):,} æ¡")
            print(f"èšåˆæ–¹å¼: {agg_way}, èšåˆå‡½æ•°: {agg_func}")
            
            # æ·»åŠ èšåˆç»“æœæç¤º
            if len(aggregated) != len(data):
                print(f"  âœ… èšåˆå®Œæˆ: {len(data):,} -> {len(aggregated):,} æ¡ (å‡å°‘ {((len(data) - len(aggregated)) / len(data) * 100):.1f}%)")
            else:
                print(f"  âš ï¸  èšåˆæœªç”Ÿæ•ˆ: æ•°æ®é‡æœªå˜åŒ– ({len(data):,} -> {len(aggregated):,} æ¡)")
            
            print(f"  âœ… èšåˆæ“ä½œå®Œæˆ!")
            return aggregated
        except Exception as e:
            print_error(f"æ•°æ®èšåˆå¤±è´¥: {e}")
            print(f"  âŒ èšåˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®æ ¼å¼ã€æ—¶é—´å­—æ®µæ ¼å¼ã€èšåˆé…ç½®ç­‰")
            return data
    
    def process_data(self, file_path=None, data=None):
        """
        å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            data: è¾“å…¥æ•°æ®ï¼Œå¦‚æœæä¾›åˆ™å¿½ç•¥file_path
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        print_header("é«˜çº§æ•°æ®å¤„ç†", "å¤§æ•°æ®é‡å¤„ç†")
        
        # åŠ è½½æ•°æ®
        if data is not None:
            self.original_data = data
            print_success(f"ä½¿ç”¨æä¾›çš„æ•°æ®ï¼ŒåŸå§‹æ•°æ®é‡: {len(data):,} æ¡")
        elif file_path is not None:
            if not self.load_data(file_path):
                return None
        else:
            print_error("è¯·æä¾›æ•°æ®æ–‡ä»¶è·¯å¾„æˆ–æ•°æ®")
            return None
        
        data = self.original_data.copy()
        original_count = len(data)
        
        # è®°å½•å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            "åŸå§‹æ•°æ®é‡": original_count,
            "å¤„ç†æ­¥éª¤": []
        }
        
        # 1. æ—¶é—´èŒƒå›´é‡‡æ ·
        if self.config["æ•°æ®é‡‡æ ·"]["æ—¶é—´èŒƒå›´é‡‡æ ·"]["å¯ç”¨"]:
            data = self.time_range_sampling(data)
            self.processing_stats["å¤„ç†æ­¥éª¤"].append({
                "æ­¥éª¤": "æ—¶é—´èŒƒå›´é‡‡æ ·",
                "æ•°æ®é‡": len(data)
            })
        
        # 2. æ•°æ®é‡‡æ ·
        if self.config["æ•°æ®é‡‡æ ·"]["å¯ç”¨é‡‡æ ·"]:
            sample_method = self.config["æ•°æ®é‡‡æ ·"]["é‡‡æ ·æ–¹å¼"]
            if sample_method == "random":
                data = self.random_sampling(data)
            elif sample_method == "systematic":
                data = self.systematic_sampling(data)
            
            self.processing_stats["å¤„ç†æ­¥éª¤"].append({
                "æ­¥éª¤": f"{sample_method}é‡‡æ ·",
                "æ•°æ®é‡": len(data)
            })
        
        # 3. å¼‚å¸¸å€¼å¤„ç†
        if self.config["æ•°æ®è¿‡æ»¤"]["å¼‚å¸¸å€¼å¤„ç†"]["å¯ç”¨"]:
            data = self.handle_outliers(data)
            self.processing_stats["å¤„ç†æ­¥éª¤"].append({
                "æ­¥éª¤": "å¼‚å¸¸å€¼å¤„ç†",
                "æ•°æ®é‡": len(data)
            })
        
        # 4. ç¼ºå¤±å€¼å¤„ç†
        if self.config["æ•°æ®è¿‡æ»¤"]["ç¼ºå¤±å€¼å¤„ç†"]["å¯ç”¨"]:
            data = self.handle_missing_values(data)
            self.processing_stats["å¤„ç†æ­¥éª¤"].append({
                "æ­¥éª¤": "ç¼ºå¤±å€¼å¤„ç†",
                "æ•°æ®é‡": len(data)
            })
        
        # 5. é‡å¤å€¼å¤„ç†
        if self.config["æ•°æ®è´¨é‡"]["é‡å¤å€¼å¤„ç†"]["å¯ç”¨"]:
            data = self.remove_duplicates(data)
            self.processing_stats["å¤„ç†æ­¥éª¤"].append({
                "æ­¥éª¤": "é‡å¤å€¼å¤„ç†",
                "æ•°æ®é‡": len(data)
            })
        
        # 6. æ•°æ®èšåˆ
        if self.config["æ•°æ®èšåˆ"]["å¯ç”¨èšåˆ"]:
            print_info("æ‰§è¡Œæ•°æ®èšåˆ...")
            print(f"  ğŸ”„ èšåˆåŠŸèƒ½å·²å¯ç”¨")
            print(f"  ğŸ“… èšåˆæ–¹å¼: {self.config['æ•°æ®èšåˆ']['èšåˆæ–¹å¼']}")
            print(f"  ğŸ§® èšåˆå‡½æ•°: {self.config['æ•°æ®èšåˆ']['èšåˆå‡½æ•°']}")
            print(f"  ğŸ•’ æ—¶é—´å­—æ®µ: {self.config['æ•°æ®èšåˆ']['æ—¶é—´å­—æ®µ']}")
            
            original_count_before_aggregate = len(data)
            data = self.aggregate_data(data)
            
            # èšåˆåçš„æç¤º
            if len(data) != original_count_before_aggregate:
                print(f"  âœ… èšåˆå®Œæˆ: {original_count_before_aggregate:,} -> {len(data):,} æ¡ (å‡å°‘ {((original_count_before_aggregate - len(data)) / original_count_before_aggregate * 100):.1f}%)")
            else:
                print(f"  âš ï¸  èšåˆæœªç”Ÿæ•ˆ: æ•°æ®é‡æœªå˜åŒ– ({original_count_before_aggregate:,} -> {len(data):,} æ¡)")
            
            self.processing_stats["å¤„ç†æ­¥éª¤"].append({
                "æ­¥éª¤": "æ•°æ®èšåˆ",
                "æ•°æ®é‡": len(data),
                "èšåˆæ–¹å¼": self.config["æ•°æ®èšåˆ"]["èšåˆæ–¹å¼"],
                "èšåˆå‡½æ•°": self.config["æ•°æ®èšåˆ"]["èšåˆå‡½æ•°"]
            })
        else:
            print_info("è·³è¿‡æ•°æ®èšåˆ (èšåˆåŠŸèƒ½æœªå¯ç”¨)")
            print(f"  â­ï¸  èšåˆåŠŸèƒ½å·²ç¦ç”¨ï¼Œä¿æŒåŸå§‹æ•°æ®é‡: {len(data):,} æ¡")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        if self.config["è¾“å‡ºé…ç½®"]["ä¿å­˜å¤„ç†åçš„æ•°æ®"]:
            self.save_processed_data(data)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.processing_stats["æœ€ç»ˆæ•°æ®é‡"] = len(data)
        self.processing_stats["æ•°æ®å‡å°‘æ¯”ä¾‹"] = (original_count - len(data)) / original_count
        
        self.processed_data = data
        
        print_success("æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"åŸå§‹æ•°æ®é‡: {original_count:,} æ¡")
        print(f"å¤„ç†åæ•°æ®é‡: {len(data):,} æ¡")
        print(f"æ•°æ®å‡å°‘æ¯”ä¾‹: {self.processing_stats['æ•°æ®å‡å°‘æ¯”ä¾‹']:.1%}")
        
        return data
    
    def save_processed_data(self, data):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            data: å¤„ç†åçš„æ•°æ®
        """
        output_config = self.config["è¾“å‡ºé…ç½®"]
        output_format = output_config["è¾“å‡ºæ ¼å¼"]
        output_dir = Path(output_config["è¾“å‡ºç›®å½•"])
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
        filename = "processed_data"
        
        try:
            if output_format == "csv":
                file_path = output_dir / f"{filename}.csv"
                data.to_csv(file_path, index=False, encoding='utf-8')
            elif output_format == "parquet":
                file_path = output_dir / f"{filename}.parquet"
                data.to_parquet(file_path, index=False)
            elif output_format == "pickle":
                file_path = output_dir / f"{filename}.pkl"
                data.to_pickle(file_path)
            
            print_success(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {file_path}")
            
            # ä¿å­˜å¤„ç†ç»Ÿè®¡ä¿¡æ¯
            stats_file = output_dir / f"{filename}_stats.json"
            write_json(self.processing_stats, stats_file)
            print_info(f"å¤„ç†ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
            
        except Exception as e:
            print_error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def get_processing_stats(self):
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        return self.processing_stats


def process_large_dataset(file_path, config=None):
    """
    å¤„ç†å¤§æ•°æ®é›†çš„ä¾¿æ·å‡½æ•°
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        config: å¤„ç†é…ç½®
        
    Returns:
        pd.DataFrame: å¤„ç†åçš„æ•°æ®
    """
    processor = AdvancedDataProcessor(config)
    return processor.process_data(file_path)


def process_dataframe(data, config=None):
    """
    å¤„ç†DataFrameçš„ä¾¿æ·å‡½æ•°
    
    Args:
        data: è¾“å…¥DataFrame
        config: å¤„ç†é…ç½®
        
    Returns:
        pd.DataFrame: å¤„ç†åçš„æ•°æ®
    """
    processor = AdvancedDataProcessor(config)
    return processor.process_data(data=data) 